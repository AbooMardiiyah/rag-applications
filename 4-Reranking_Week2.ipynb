{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62aa4434-792c-49c0-a7f9-2326599231fe",
   "metadata": {},
   "source": [
    "#### Week 2: Vector Search Applications w/ LLMs.  Authored by Chris Sanchez.\n",
    "\n",
    "# Week 2 - Notebook 4\n",
    "\n",
    "# Overview\n",
    "We will divide the approach to building our system into four parts over two weeks-\n",
    "\n",
    "#### Week One\n",
    "* ~Part 1:~\n",
    "  * ~Data ingest and preprocessing~\n",
    "  * ~Convert text into vectors~\n",
    "* ~Part 2~:\n",
    "  * ~Index data on Weaviate database~\n",
    "  * ~Search over data~\n",
    "* ~Part 2.5~:\n",
    "  * ~Benchmark retrieval results~\n",
    "\n",
    "#### Week Two\n",
    "* Part 3 **(THIS NOTEBOOK)**:\n",
    "  * Add a reranker to the mix (new benchmark)\n",
    "* Part 4:\n",
    "  * Integrate with GPT-Turbo\n",
    "* Part 5:\n",
    "  * Display results in Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00939c5e-2070-4119-9631-eabca0e4000e",
   "metadata": {},
   "source": [
    "#### This notebook will cover the highlighted portion of the technical diagram below, as initially referenced in the Course content:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112de549-0e64-4199-b91e-edb7b476eca1",
   "metadata": {},
   "source": [
    "# PLACEHOLDER CELL FOR ARCH DIAGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9299e2fb-9f95-4690-b172-0d6e9b4952cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from weaviate_interface import WeaviateClient\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from reranker import ReRanker\n",
    "import pandas as pd\n",
    "from rich import print  # nice library that provides improved printing output (overrides default print function)\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "#load from local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from preprocessing import FileIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64b118-430d-41ec-95c8-317c7a0e8b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dab4953-0e81-4f53-9708-ee91537211a4",
   "metadata": {},
   "source": [
    "# Assignment 2.3 - Build a Hybrid Search method\n",
    "**Implement a hybrid search method on the Weaviate python client**. \n",
    "\n",
    "#### Instructions:\n",
    "- Fill in the areas of the code wherever you see a `None` statement.\n",
    "- `fusion_type` is one of the hybrid search parameters.  Leave this value as [**`relativeScoreFusion`**](https://weaviate.io/blog/hybrid-search-fusion-algorithms) in your code, doing so will ensure that you are using Weaviate's preferred ranking alogrithm for hybrid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bb185-9878-4973-ba2f-0a9c3b9abfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure search constants\n",
    "class_name = None\n",
    "query = None\n",
    "query_embedding = None\n",
    "\n",
    "#design hybird search query\n",
    "'''\n",
    "Get objects using bm25 and vector, then combine the results using a reciprocal ranking algorithm.\n",
    "\n",
    "Args\n",
    "----\n",
    "query: str\n",
    "    User query.\n",
    "class_name: str\n",
    "    Class (index) to search.\n",
    "properties: List[str]\n",
    "    List of properties to search across (using BM25)\n",
    "alpha: float=0.5\n",
    "    Weighting factor for BM25 and Vector search.\n",
    "    alpha can be any number from 0 to 1, defaulting to 0.5:\n",
    "        alpha = 0 executes a pure keyword search method (BM25)\n",
    "        alpha = 0.5 weighs the BM25 and vector methods evenly\n",
    "        alpha = 1 executes a pure vector search method\n",
    "limit: int=10\n",
    "    Number of results to return.\n",
    "display_properties: List[str]=None\n",
    "    List of properties to return in response.\n",
    "    If None, returns all properties.\n",
    "'''\n",
    "\n",
    "response = (client.query\n",
    "            \n",
    " .get(None, None). #reminder that the properties param here refers to the \"display_properties\"\n",
    "            \n",
    " # use near_vector our search method, and only search over the \"content\" property\n",
    " .with_hybrid(query=None,\n",
    "              alpha=None,\n",
    "              vector=None,\n",
    "              properties=None,\n",
    "              fustion_type='relativeScoreFusion')\n",
    "\n",
    " # instead of \"score\", vector search can return a \"distance\" property for scoring, the smaller the distance, the more semantically similar is the result\n",
    " .with_additional(['score', 'distance', 'explainScore'])\n",
    "            \n",
    " # limit the returned results to the top-3 ranked hits\n",
    " .with_limit(None)\n",
    "            \n",
    " # execute the search with the \"do\" command\n",
    " .do()\n",
    ")\n",
    "\n",
    "# To show cleaned up results we'll use the built-in format response method\n",
    "print(client.format_response(response, class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e934fe6-53e2-4720-8583-ba93018393b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReRanker:\n",
    "    '''\n",
    "    Cross-Encoder models achieve higher performance than Bi-Encoders, \n",
    "    however, they do not scale well to large datasets. The lack of scalability\n",
    "    is due to the underlying cross-attention mechanism, which is computationally\n",
    "    expensive.  Thus a Bi-Encoder is best used for 1st-stage document retrieval and \n",
    "    a Cross-Encoder is used to re-rank the retrieved documents. \n",
    "\n",
    "    https://www.sbert.net/examples/applications/cross-encoder/README.html\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_name: str='cross-encoder/ms-marco-MiniLM-L-6-v2', local_files: bool=False):\n",
    "        self.model_name = model_name\n",
    "        self.model = CrossEncoder(self.model_name, automodel_args={'local_files_only':local_files})\n",
    "        self.score_field = 'cross_score'\n",
    "\n",
    "    def _cross_encoder_score(self, \n",
    "                             results: List[dict], \n",
    "                             query: str, \n",
    "                             cross_score_key: str='cross-score',\n",
    "                             return_scores: bool=False\n",
    "                             ) -> Union[np.array, None]:\n",
    "        '''\n",
    "        Given a list of hits from a Retriever:\n",
    "            1. Scores hits by passing query and results through CrossEncoder model. \n",
    "            2. Adds cross-score key to hits dictionary. \n",
    "            3. If desired returns np.array of Cross Encoder scores.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        results: List[dict]\n",
    "            List of search results from OpenSearch client.\n",
    "        query: str\n",
    "            User query.\n",
    "        cross_score_key: str='cross-score'\n",
    "            Name of key/field that the new calculated cross encoder score will be associated with.\n",
    "        return_scores: bool=False\n",
    "            If True, returns a np.array of cross encoder scores. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Either returns a np.array of cross encoder scores if \"return_scores\" is True, otherwise\n",
    "        nothing is returned.  The primary purpose of this function is to update the \"results\" dict. \n",
    "        '''\n",
    "        \n",
    "        #build query/content list \n",
    "        #create a list of lists that contains the query and the content field of each result from \"results\"\n",
    "        #important the cross_input variable must be a list of lists\n",
    "        cross_input = None\n",
    "        \n",
    "        #get scores\n",
    "        # Call the self.model's predict method to get predicted scores on the cross_input\n",
    "        # Output at this step will be a numpy matrix of cross-encoder scores\n",
    "        ######################################################################\n",
    "        # Example: \n",
    "        # array([ 1.3296969 ,  0.8297793 ,  1.2054391 ,  2.9448447 ,  2.7284985 ,\n",
    "        #         4.231843  , -1.6208533 ,  2.4096487 , -1.2081863 ,  2.9743905 ,\n",
    "        #         3.2194595 , -0.27501446,  1.5256095 ,  2.8193645 ,  1.5568736 ,\n",
    "        #         2.5138354 ,  1.9419916 ,  2.6341028 , -1.6115644 , -0.49818742,\n",
    "        #         3.695484  ,  2.93317   ,  3.1728778 , -0.5114989 , -4.076729  ], dtype=float32)\n",
    "\n",
    "        cross_scores = None\n",
    "\n",
    "        #enumerate through the results and update each dict with the cross_score_key arg as key and value as the new score:\n",
    "        #Example:\n",
    "             # {'cross-score' : 5.12345}\n",
    "        for i, result in enumerate(results):\n",
    "            None\n",
    "\n",
    "        if return_scores:\n",
    "            return cross_scores\n",
    "\n",
    "    def rerank(self, results: List[dict], query: str, top_k: int=10, threshold: float=None) -> List[dict]:\n",
    "        '''\n",
    "        Given a list of search results from OpenSearch client, results are scored with a Cross Encoder \n",
    "        and returned in sorted order by the cross_score field.  Threshold allows user to filter out \n",
    "        results that do not meet cross_score threshold value:\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        results: List[dict]\n",
    "            List of search results from OpenSearch client.\n",
    "        query: str\n",
    "            User query.\n",
    "        top_k: int=10\n",
    "            Number of reranked results to return\n",
    "        threshold: float=None\n",
    "            If None, top_k results will be returned.  \n",
    "            If float value is present, only results with a cross_score that meet or exceed the threshold\n",
    "            will be retuned.  This arg is present to prevent very low scoring document from being returned. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of reranked search results. \n",
    "        '''\n",
    "        # call the internal _cross_encoder_score function (it's ok that nothing is returned here)\n",
    "        # the results dictionary is being updated \n",
    "        None\n",
    "\n",
    "        #sort results by the new cross-score field\n",
    "        sorted_hits = None\n",
    "\n",
    "        #if user wants to set a threshold we need to account for that\n",
    "        if threshold or threshold == 0:\n",
    "\n",
    "            #filter sorted_hits by the threshold value\n",
    "            filtered_hits = None\n",
    "            \n",
    "            if not any(filtered_hits):\n",
    "                logger.warning(f'No hits above threshold {threshold}. Returning top {top_k} hits.')\n",
    "                return sorted_hits[:top_k]\n",
    "            return filtered_hits\n",
    "            \n",
    "        #if no threshold was set return top_k sorted_hits\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d905c45-88ce-49a6-8ff7-fb7ae5934fac",
   "metadata": {},
   "source": [
    "### Instantiate the ReRanker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a4bf326-c463-4f82-a7ef-3585d66c23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d46d8fc-5bc8-42d2-8dae-ebe8c69cad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3296969 ,  0.8297793 ,  1.2054391 ,  2.9448447 ,  2.7284985 ,\n",
       "        4.231843  , -1.6208533 ,  2.4096487 , -1.2081863 ,  2.9743905 ,\n",
       "        3.2194595 , -0.27501446,  1.5256095 ,  2.8193645 ,  1.5568736 ,\n",
       "        2.5138354 ,  1.9419916 ,  2.6341028 , -1.6115644 , -0.49818742,\n",
       "        3.695484  ,  2.93317   ,  3.1728778 , -0.5114989 , -4.076729  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker._cross_encoder_score(response, query=query, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952a99df-a089-4fd8-b76d-2294fe70777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index name\n",
    "index_name = 'impact-theory-minilm-196'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7819bf44-faac-437f-8599-a44bb9b40040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set query\n",
    "query = \"how do I change my life for good\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8f05d-1503-4bed-b795-58166745e436",
   "metadata": {},
   "source": [
    "### Test ReRanker class by conducting a hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bd521-1f7c-49e2-aa66-0df7e3742cf9",
   "metadata": {},
   "source": [
    "# Evaluation of Reranker Effect on Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8597d6-fb9a-4073-afd8-88b4dbab3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_search(limit: int, rerank: bool=True):\n",
    "    start = time.perf_counter()\n",
    "    response = client.keyword_search('how do I make a million dollars', 'Impact_theory_minilm_256', limit=limit, display_properties=['content', 'title'])\n",
    "    reranked = reranker.rerank(response)\n",
    "    end = time.perf_counter() - start\n",
    "    return round(end, 3)\n",
    "\n",
    "false_times = []\n",
    "for x in tqdm(range(10, 400, 10)):\n",
    "    false_times.append((time_search(x, rerank=False), x))\n",
    "    \n",
    "\n",
    "ranked_times = []\n",
    "for x in tqdm(range(10, 400, 10)):\n",
    "    ranked_times.append((time_search(x, rerank=True), x))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "false_df = pd.DataFrame(false_times, columns=['time', 'n'])\n",
    "ranked = pd.DataFrame(ranked_times, columns=['time', 'n'])\n",
    "\n",
    "ax = false_df.plot.scatter(x='n', y='time', label='No Reranker')\n",
    "ax2 = ranked.plot.scatter(x='n', y='time', ax=ax, color='orange', ylabel='Latency (ms)', label='With Reranker', xlabel='# of Returned Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eaa403-1a82-4555-9205-d6ca8528bdbf",
   "metadata": {},
   "source": [
    "# Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e8fd66-2b28-40bf-b866-a923862e4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/valid_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea62a2-f8fe-4dac-ba2e-b5544b1aa851",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
