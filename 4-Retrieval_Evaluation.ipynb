{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88728355-dd6c-4c08-8ad8-ac7f85e6af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f03b28-68e1-45c5-9ec0-99d9a32d0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#external files\n",
    "from preprocessing import FileIO\n",
    "from openai_interface import GPT_Turbo\n",
    "from opensearch_interface import OpenSearchClient\n",
    "from reranker import ReRanker\n",
    "\n",
    "#standard library imports\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from typing import List, Any, Dict, Tuple, Union\n",
    "\n",
    "#misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bb049-ca24-42ef-9458-72fb687fe27d",
   "metadata": {},
   "source": [
    "### Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e61a946-f5b2-4a7e-80b6-1dc5bdaa4cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (37007, 16)\n",
      "Memory Usage: 4.27+ MB\n"
     ]
    }
   ],
   "source": [
    "data_path = './practice_data/impact_theory_minilm_196.parquet'\n",
    "data = FileIO().load_parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a130cd-03de-4390-9882-3189c2ca9384",
   "metadata": {},
   "source": [
    "### Randomly select 100 chunks for Q/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c379516f-ee58-4445-8d2e-0ec8241bb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b33379-f58b-45d9-9c72-d31f0537149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data: List[dict], sample_size: int):\n",
    "    sample = random.sample(data, sample_size)\n",
    "    contents = [(d['doc_id'], d['content']) for d in sample]\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8dc7bf0-8f53-4603-ba22-d05a7a6dd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(sample: List[dict], key: str=\"doc_id\") -> List[Any]:\n",
    "    return [d[key] for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89f27c6-87ca-4481-a74d-b11044a1ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(doc_id: str, corpus: List[dict], full_dict: bool=False):\n",
    "    result = [d for d in corpus if d['doc_id'] == doc_id][0]\n",
    "    if full_dict: return result\n",
    "    else: return result['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71526d96-b5d4-4ce0-8e18-5b5c64eea9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that would be nice if we all thought that way But of course, there's this primitive mind that we talked about which is this part of your brain that is not wired for truth it's wired for survival in 50,000 BC and what that often meant was agreeing with The sacred beliefs of your tribe and believing them and the people who could believe what the tribe believed With full conviction they survived. Well, they were you know, they were on the in-group they fit in and that's what was needed What's up, guys?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample('kE3yryW-FiE_33', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bda73ab-2994-4ce6-83c3-9b6aefcd8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_numbers(query: str):\n",
    "    return query[3:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4919dee7-a38d-4c82-9cc6-ed9a58b9bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions(question_tuples: List[tuple]) -> Dict[str, List[str]]:\n",
    "    question_dict = {}\n",
    "    for tup in question_tuples:\n",
    "        doc_id = tup[0]\n",
    "        questions = tup[1].split('\\n')\n",
    "        questions = [strip_numbers(q) for q in questions]\n",
    "        question_dict[doc_id] = questions\n",
    "    return question_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53289eed-4590-4c30-a840-30c329ecb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data: List[dict], dir_path: str, num_questions: int=100, batch_size: int=50):\n",
    "    gpt = GPT_Turbo()\n",
    "    if batch_size > 50:\n",
    "        raise ValueError('Due to OpenAI rate limits, batch_size cannot be greater than 50')\n",
    "\n",
    "    time_marker = datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
    "    filepath = os.path.join(dir_path, f\"{num_questions}_questions_{time_marker}.json\")\n",
    "    \n",
    "    sample = sample_data(data, num_questions)\n",
    "    batches = ceil(num_questions/batch_size)\n",
    "    all_questions = []\n",
    "    for n in range(batches):\n",
    "        batch = sample[n*batch_size:(n+1)*batch_size]\n",
    "        questions = gpt.batch_generate_question_context_pairs(batch)\n",
    "        all_questions.append(questions)\n",
    "        if n < batches - 1:\n",
    "            print('Pausing for 60 seconds due to OpenAI rate limits...')\n",
    "            time.sleep(60)\n",
    "    all_questions = [tup for batch in all_questions for tup in batch]\n",
    "    processed_questions = process_questions(all_questions)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(processed_questions, f, indent=4)\n",
    "    return processed_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b797f1-2642-4285-86d1-cb197a9811c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:07<00:00,  6.59Generated Questions/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing for 60 seconds due to OpenAI rate limits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:07<00:00,  6.79Generated Questions/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset(data=data, dir_path='./practice_data/', num_questions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a440bd3e-1223-41c4-ad1a-0120273d9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gteclient = OpenSearchClient(model_name_or_path='/home/elastic/notebooks/vector_search_applications/models/gte-base/')\n",
    "osclient = OpenSearchClient()\n",
    "reranker = ReRanker()\n",
    "intfloat = ReRanker(model_name='intfloat/simlm-msmarco-reranker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd43eef-e71d-4713-8664-6ebd14f076f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did the United States respond to the Soviet Union's advancements in space?\"\n",
    "kw_index = 'impact-theory-minilm-196'\n",
    "vec_index = 'impact-theory-minilm-196'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "255bdabd-770e-4b12-8bd8-54e5cdc64f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation( dataset: Dict[str, List[str]], \n",
    "                    retriever: OpenSearchClient,\n",
    "                    reranker: ReRanker,\n",
    "                    kw_index_name: str, \n",
    "                    vector_index_name: str,\n",
    "                    response_size: int=10,\n",
    "                    top_k: int=5,\n",
    "                    chunk_size: int=196,\n",
    "                    rerank_all_responses: bool=False,\n",
    "                    ) -> Tuple[int, int, int, int]:\n",
    "\n",
    "    top_k = top_k if top_k else response_size\n",
    "    reranker_name = reranker.model_name if rerank_all_responses else \"None\"\n",
    "    \n",
    "    results_dict = {'n':response_size, \n",
    "                    'top_k': top_k, \n",
    "                    'Retriever': retriever.model_name_or_path, \n",
    "                    'Ranker': reranker_name,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'kw_hit_rate': 0,\n",
    "                    'vector_hit_rate': 0,\n",
    "                    'hybrid_hit_rate':0,\n",
    "                    'total_questions':0\n",
    "                    }\n",
    "    for doc_id, questions in tqdm(dataset.items(), 'Questions'):\n",
    "        for q in questions:\n",
    "            results_dict['total_questions'] += 1\n",
    "            \n",
    "            #make calls to OpenSearch host of: Keyword, Vector, and Hybrid\n",
    "            kw_response = retriever.keyword_search(query=q, index=kw_index_name, size=response_size)\n",
    "            vector_response = retriever.vector_search(query=q, index=vector_index_name, size=response_size)\n",
    "            hybrid_response = retriever.hybrid_search(q, kw_index_name, vector_index_name, kw_size=response_size, vec_size=response_size)\n",
    "\n",
    "            #rerank returned responses if rerank_all is True\n",
    "            if rerank_all_responses:\n",
    "                kw_response = reranker.rerank(kw_response, q, top_k=top_k)\n",
    "                vector_response = reranker.rerank(vector_response, q, top_k=top_k)\n",
    "                hybrid_response = reranker.rerank(hybrid_response, q, top_k=top_k)\n",
    "                \n",
    "            #collect doc_ids to check for document matches (include only top_k if top_k > 0)\n",
    "            kw_doc_ids = [res['_source']['doc_id'] for res in kw_response][:top_k]\n",
    "            vector_doc_ids = [res['_source']['doc_id'] for res in vector_response][:top_k]\n",
    "            hybrid_doc_ids = [res['_source']['doc_id'] for res in hybrid_response][:top_k]\n",
    "            \n",
    "            #increment hit_rate counters as appropriate\n",
    "            if doc_id in kw_doc_ids:\n",
    "                results_dict['kw_hit_rate'] += 1\n",
    "            if doc_id in vector_doc_ids:\n",
    "                results_dict['vector_hit_rate'] += 1\n",
    "            if doc_id in hybrid_doc_ids:\n",
    "                results_dict['hybrid_hit_rate'] += 1\n",
    "\n",
    "    #use raw counts to calculate final scores\n",
    "    calc_hit_rate_scores(results_dict)\n",
    "    \n",
    "    return results_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a94f824a-1bd6-4e9f-b325-fa4a44e529ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hit_rate_scores(results_dict: Dict[str, Union[str, int]]) -> None:\n",
    "    for prefix in ['kw', 'vector', 'hybrid']:\n",
    "        results_dict[f'{prefix}_score'] = round(results_dict[f'{prefix}_hit_rate']/results_dict['total_questions'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a96addfe-117c-4599-b5dc-e00a22e664cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_results(results_dict: Dict[str, Union[str, int]], dir_outpath: str=None) -> None:\n",
    "    #write results to output file\n",
    "    if dir_outpath:\n",
    "        time_marker = datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
    "        path = os.path.join(dir_outpath, f'retrieval_eval_{chunk_size}_{time_marker}.json')\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "864cb7bc-6c60-4f0b-a15a-37c440f3fdca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intfloat/simlm-msmarco-reranker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:11<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 196\n",
    "all_results = []\n",
    "for x in range(60,61):\n",
    "    results = run_evaluation( dataset=dataset, \n",
    "                              retriever=osclient, \n",
    "                              reranker=intfloat,\n",
    "                              kw_index_name=kw_index, \n",
    "                              vector_index_name=vec_index, \n",
    "                              response_size=x, \n",
    "                              top_k=10,\n",
    "                              rerank_all_responses=True,\n",
    "                            )\n",
    "    all_results.append(results)\n",
    "record_results(all_results, dir_outpath='./practice_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d1ae8bd-71a6-42bf-b1cb-ea248f9cc029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"\\xe9.^%\\x06\\x19\\xe9\\x0e\\xd3\\xc7]\\x86\\xfb\\x82\\x88;\\x86C\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\"]\n",
      "Bad pipe message: %s [b'\\x07Co']\n",
      "Bad pipe message: %s [b'dEG[\\x11\\x96\\x16\\xd0X\\xbd\\r\\xde{\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0']\n",
      "Bad pipe message: %s [b\"$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\"]\n",
      "Bad pipe message: %s [b'\\xf8\\xb4\\\\\\x07n\\x80\\xed\\x8d\\x7fit\\xf2\\x89r\\xcd\\xb9aV\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00']\n",
      "Bad pipe message: %s [b'\\n\\x00#\\x00\\x00\\x00\\x0f\\x00']\n",
      "Bad pipe message: %s [b'T\\x1a\\x9c\\rj\\x10\\xd8\\xed\\x02\\x90\\x1a\\x0c\\xef\\xd8\\xa9\\xd6\\xba\\x01\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00', b'\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b'g\\xc3[\\x02\\xc4U\\xc7\\xe7gO\\xb9\\x8d\\xe1\\x1bT,\\xf9\\x8d\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00', b'E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c']\n",
      "Bad pipe message: %s [b'b\\xf1\\x12T\\x01\\xa8d\\x89\\xa8\"\\xb72\\x0b\\xc6\\xaa\\x88v \\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0\\'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00']\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0']\n",
      "Bad pipe message: %s [b'\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00', b' \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05']\n",
      "Bad pipe message: %s [b'\\x03', b'\\x04\\x02\\x04', b'\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# query = 'How do I get ahead in life?'\n",
    "# resp = osclient.hybrid_search(query, kw_index, vec_index, kw_size=60, vec_size=60)\n",
    "# intfloat.rerank(resp, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6220d-fde4-4cb2-af83-e08167ae7548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
