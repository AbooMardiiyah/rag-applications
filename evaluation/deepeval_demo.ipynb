{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424b4361-f20c-4f54-afff-7ba76578667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "envs = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7dba2cec-e732-4a67-9e59-bfaf14d2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import get_token_count\n",
    "from src.llm.prompt_templates import question_answering_prompt_series, huberman_system_prompt\n",
    "from app_features import generate_prompt_series\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from litellm import ModelResponse\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f89c8ad-7fb8-4982-accd-dba8436fc84e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 1, strict: True, evaluation model: gpt-4, reason: The score is 1.00 because the response perfectly addressed the concern raised in the input without any irrelevant statements., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra costs.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra costs.']\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x7f40b86ea920>], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra costs.', expected_output=None, context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra costs.'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model='gpt-4', strict_mode=True)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    ")\n",
    "evaluate([test_case], [answer_relevancy_metric], run_async=False, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4ebecf-5553-4707-8ee8-1e35fc1fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Give a brief explanation of how brain neuroplasticity works\",\n",
    "             \"What is the role of dopamine in the body\",\n",
    "             \"What is a catecholimine\",\n",
    "             \"What does Jocko have to say about leadership\",\n",
    "             \"What does Fridman think about the evolution of AI\", \n",
    "             \"Who is the host of the Huberman Labs podcast\",\n",
    "             \"Why do people make self-destructive decisions\",\n",
    "             \"Provide better sleep protocol in list format\",\n",
    "             \"What are the topcis that Lex Fridman discusses\",\n",
    "             \"Is there a generally positive outlook on the future of AI\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "444bf30f-9a5b-4994-8a77-3c0ef571e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_weaviate_client()\n",
    "turbo = LLM(model_name='gpt-3.5-turbo-0125')\n",
    "collection_name = 'Huberman_minilm_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7aca859-f202-48e9-b329-e39ca6505db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_bundle(query: str,\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      answer_llm: LLM,\n",
    "                      ground_truth_llm: LLM=None\n",
    "                     ) -> tuple[str, list[list[str]], str]:\n",
    "    '''\n",
    "    Returns answer, ground truth and associated context from a single query.\n",
    "    '''\n",
    "    def format_llm_response(response: ModelResponse) -> str:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    #1st-stage retrieval (get contexts)\n",
    "    context = client.hybrid_search(query, collection_name, \n",
    "                                   query_properties=['content', 'title', 'short_description'],\n",
    "                                   limit=3, \n",
    "                                   return_properties=['content', 'guest', 'short_description'])\n",
    "    #create contexts from content field\n",
    "    contexts = [d['content'] for d in context]\n",
    "    \n",
    "    #generate assistant message prompt\n",
    "    assist_message = generate_prompt_series(query, context, summary_key='short_description')\n",
    "\n",
    "    #generate answers from model being evaluated\n",
    "    answer = format_llm_response(answer_llm.chat_completion(huberman_system_prompt, assist_message))\n",
    "\n",
    "    #create ground truth answers\n",
    "    if ground_truth_llm:\n",
    "        ground_truth = format_llm_response(ground_truth_llm.chat_completion(huberman_system_prompt, assist_message))\n",
    "        return query, contexts, answer, ground_truth\n",
    "    return query, contexts, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6a84919-1f2d-43e9-9527-aa3b81ae07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from time import sleep\n",
    "\n",
    "async def create_test_dataset(questions: list[str], \n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              answer_llm: LLM,\n",
    "                              ground_truth_llm: LLM=None, \n",
    "                              batch_size: int=5, \n",
    "                              async_mode: bool=True,\n",
    "                              disable_internal_tqdm: bool=False):\n",
    "    total = len(questions)\n",
    "    progress = tqdm('Queries', total=total, disable=disable_internal_tqdm)\n",
    "    data = []\n",
    "    batches = ceil(total/batch_size)\n",
    "    for i in range(batches):\n",
    "        batch = questions[i*batch_size:(i+1)*batch_size]\n",
    "        if async_mode:\n",
    "            results = await asyncio.gather(*[aget_answer_bundle(query, \n",
    "                                                                client, \n",
    "                                                                collection_name, \n",
    "                                                                answer_llm,\n",
    "                                                                ground_truth_llm) for query in batch])\n",
    "            if any(results):\n",
    "                data.extend(results)\n",
    "            else:\n",
    "                raise \"No results returned for initial batch, double-check your inputs.\"\n",
    "        else:\n",
    "            with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "                futures = [executor.submit(get_answer_bundle, query, client, collection_name, answer_llm, ground_truth_llm) for query in batch]\n",
    "                for future in as_completed(futures):\n",
    "                    progress.update(1)\n",
    "                    data.append(future.result())\n",
    "        print(f\"Finished with batch {i+1}, taking a break...\")\n",
    "    asyncio.\n",
    "    queries = [d[0] for d in data]\n",
    "    contexts = [d[1] for d in data]\n",
    "    answers = [d[2] for d in data]\n",
    "    if len(data[0]) == 4:\n",
    "        ground_truths = [d[3] for d in data]\n",
    "        return queries, contexts, answers, ground_truths\n",
    "    return queries, contexts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5abd639-e47b-44bc-971f-8602ab30f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with batch 1, taking a break...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/10 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with batch 2, taking a break...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = await create_test_dataset(questions, client, collection_name, turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48edd30-c6b9-40be-80cd-cf43c25f2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aget_answer_bundle( query: str,\n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              answer_llm: LLM,\n",
    "                              ground_truth_llm: LLM=None\n",
    "                             ) -> tuple[str, list[list[str]], str]:\n",
    "    '''\n",
    "    Returns answer, ground truth and associated context from a single query.\n",
    "    '''\n",
    "    def format_llm_response(response: ModelResponse) -> str:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    #1st-stage retrieval (get contexts)\n",
    "    context = client.hybrid_search(query, collection_name, \n",
    "                                   query_properties=['content', 'title', 'short_description'],\n",
    "                                   limit=3, \n",
    "                                   return_properties=['content', 'guest', 'short_description'])\n",
    "    #create contexts from content field\n",
    "    contexts = [d['content'] for d in context]\n",
    "    \n",
    "    #generate assistant message prompt\n",
    "    assist_message = generate_prompt_series(query, context, summary_key='short_description')\n",
    "\n",
    "    #generate answers from model being evaluated\n",
    "    answer = await turbo.achat_completion(huberman_system_prompt, assist_message)\n",
    "    answer = format_llm_response(answer)\n",
    "\n",
    "    #create ground truth answers\n",
    "    if ground_truth_llm:\n",
    "        ground_truth = format_llm_response(ground_truth_llm.chat_completion(huberman_system_prompt, assist_message))\n",
    "        return query, contexts, answer, ground_truth\n",
    "    return query, contexts, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb1f8b09-ea8d-4dc1-8049-c0751c20e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = await aget_answer_bundle(questions[0], client, collection_name, turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36f4fa98-27cc-4120-9d4c-0cd3b1f57210",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, contexts, answers = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "766a8a26-c10b-4d13-af2e-9379409eadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_dataset(questions: list[str],\n",
    "                        contexts: list[list[str]],\n",
    "                        answers: list[str]\n",
    "                       ) -> EvaluationDataset:\n",
    "    assert len(questions) == len(contexts) == len(answers), 'Mismatched lengths in input values, retry after correcting'\n",
    "    test_cases = []\n",
    "    for i in range(len(questions)):\n",
    "        test_case = LLMTestCase(input=questions[i],\n",
    "                                actual_output=answers[i],\n",
    "                                retrieval_context=contexts[i])\n",
    "        test_cases.append(test_case)\n",
    "    return EvaluationDataset(alias='Initial test', test_cases=test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e70920-c764-4c44-8fd2-819bd63e0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d705726-9def-4555-8190-31c00fa9f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = create_eval_dataset(queries, contexts, answers)\n",
    "arm = AnswerRelevancyMetric(model='gpt-4', threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5beec4c7-48f3-445c-b691-121b398c8e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637fc75149c74e0a9ef4f0c1947acb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36284f2bc4324e7bbac6995f12d48987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f004577fcdf49ccb609d848416e2e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=114 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=114 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=119 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=119 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=120 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=120 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=121 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=121 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=128 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=128 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=129 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=129 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/site-packages/openai/_response.py:666: ResourceWarning: unclosed \n",
       "&lt;socket.socket fd=118, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', \n",
       "42416), raddr=('104.18.7.192', 443)&gt;\n",
       "  def wrapped(*args: P.args, **kwargs: P.kwargs) -&gt; AsyncResponseContextManager[AsyncAPIResponse[R]]:\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/site-packages/openai/_response.py:666: ResourceWarning: unclosed \n",
       "<socket.socket fd=118, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', \n",
       "42416), raddr=('104.18.7.192', 443)>\n",
       "  def wrapped(*args: P.args, **kwargs: P.kwargs) -> AsyncResponseContextManager[AsyncAPIResponse[R]]:\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=118 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=118 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da86c6cac06840df853833475da9ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b1b85b07b44c6391446115bdab68b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b54d429c054d7183a2ab5cfa456ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=118 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=118 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=119 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=119 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=121 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=121 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=127 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=127 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=128 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=128 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=130 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=130 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=120 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=120 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=131 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=131 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f3777ac44141f6a8d49352ba7294cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2943398d76dd461f96b4a7bd5dc02020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e43479167cd455e88bba99ec7ed8171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae9e4cd74b14d08b7543aa4c71bcdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = evaluate(eval_dataset, [arm], print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05793c09-1157-446f-b291-633cc947f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'_threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'using_native_model'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'evaluation_model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'include_reason'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'async_mode'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'strict_mode'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'evaluation_cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03444</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'statements'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"Neuroplasticity is the nervous system's ability to change in response to experience.\"</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'At a cellular level, this occurs through mechanisms such as long-term potentiation, which involves </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strengthening specific connections between neurons at synapses.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'This strengthening allows certain neurons to communicate more effectively with each other, enabling the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">nervous system to adapt and learn based on experiences.'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'verdicts'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AnswerRelvancyVerdict</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AnswerRelvancyVerdict</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AnswerRelvancyVerdict</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The score is 1.00 because the response perfectly addressed the question, providing a brief </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">explanation of how brain neuroplasticity works without any irrelevant information.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'_threshold'\u001b[0m: \u001b[1;36m0.7\u001b[0m,\n",
       "    \u001b[32m'using_native_model'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'evaluation_model'\u001b[0m: \u001b[32m'gpt-4'\u001b[0m,\n",
       "    \u001b[32m'include_reason'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'async_mode'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'strict_mode'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'evaluation_cost'\u001b[0m: \u001b[1;36m0.03444\u001b[0m,\n",
       "    \u001b[32m'statements'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"Neuroplasticity is the nervous system's ability to change in response to experience.\"\u001b[0m,\n",
       "        \u001b[32m'At a cellular level, this occurs through mechanisms such as long-term potentiation, which involves \u001b[0m\n",
       "\u001b[32mstrengthening specific connections between neurons at synapses.'\u001b[0m,\n",
       "        \u001b[32m'This strengthening allows certain neurons to communicate more effectively with each other, enabling the \u001b[0m\n",
       "\u001b[32mnervous system to adapt and learn based on experiences.'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'verdicts'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mAnswerRelvancyVerdict\u001b[0m\u001b[1m(\u001b[0m\u001b[33mverdict\u001b[0m=\u001b[32m'yes'\u001b[0m, \u001b[33mreason\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAnswerRelvancyVerdict\u001b[0m\u001b[1m(\u001b[0m\u001b[33mverdict\u001b[0m=\u001b[32m'yes'\u001b[0m, \u001b[33mreason\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAnswerRelvancyVerdict\u001b[0m\u001b[1m(\u001b[0m\u001b[33mverdict\u001b[0m=\u001b[32m'yes'\u001b[0m, \u001b[33mreason\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'reason'\u001b[0m: \u001b[32m'The score is 1.00 because the response perfectly addressed the question, providing a brief \u001b[0m\n",
       "\u001b[32mexplanation of how brain neuroplasticity works without any irrelevant information.'\u001b[0m,\n",
       "    \u001b[32m'success'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(vars(evaluation[0].metrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ce481-bae0-4d01-ae76-f57b36434c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
