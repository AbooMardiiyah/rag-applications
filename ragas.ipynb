{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6f9a84-66b7-4f1a-bfd4-1a1fcabc7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from llama_index import download_loader\n",
    "from ragas.testset import TestsetGenerator\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "load_dotenv('/home/elastic/notebooks/vector_search_applications/.env', override=True)\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7b940981-dc8c-4523-9974-96abd5b05209",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemanticScholarReader = download_loader(\"SemanticScholarReader\")\n",
    "loader = SemanticScholarReader()\n",
    "query_space = \"large language models\"\n",
    "documents = loader.load_data(query=query_space, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "8fca9d63-5008-424d-833f-002c9ca1634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetgenerator = TestsetGenerator.from_default()\n",
    "test_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "6e355364-f69c-4b7e-b4ba-1da14993598c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "fe2796a5-642d-4105-b409-1550e82af2e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f630930444f6493aa96f2db854f3a78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29026.33it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 24244.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12595.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29228.60it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25575.02it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13751.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29852.70it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26132.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12409.18it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13617.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22250.95it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26630.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "Text Chunks: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4161.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29433.71it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29433.71it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15196.75it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13443.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29026.33it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 23366.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15087.42it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13273.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29537.35it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15196.75it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26462.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25343.23it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27869.13it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13066.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20360.70it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10824.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28630.06it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27060.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27060.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28339.89it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26800.66it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13107.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7989.15it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26214.40it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26546.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11554.56it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13025.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28728.11it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13400.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 23899.17it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25731.93it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13273.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30066.70it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25266.89it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13617.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14768.68it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26800.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12945.38it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26214.40it/s]\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26296.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26973.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15477.14it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13797.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8224.13it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15307.68it/s]\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26715.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Chunks: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "465it [22:08,  2.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What advice would the Stoics give to someone s...</td>\n",
       "      <td>And so cognitive dissonance is a powerful forc...</td>\n",
       "      <td>The Stoics would advise someone struggling wit...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How would the Stoics approach the issue of ill...</td>\n",
       "      <td>And so cognitive dissonance is a powerful forc...</td>\n",
       "      <td>The Stoics would likely approach the issue of ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Should individuals have the right to choose CO...</td>\n",
       "      <td>So you felt that it was obviously wrong to for...</td>\n",
       "      <td>The answer is not explicitly stated in the giv...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the concern about AI bias and its imp...</td>\n",
       "      <td>But what scares me is if you plug in AI bias i...</td>\n",
       "      <td>The concern about AI bias and its impact on sh...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the proliferation of AI impact the dy...</td>\n",
       "      <td>- \"you've got a dynamic between the US and Chi...</td>\n",
       "      <td>The proliferation of AI impacts the dynamic be...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What advice would the Stoics give to someone s...   \n",
       "1  How would the Stoics approach the issue of ill...   \n",
       "2  Should individuals have the right to choose CO...   \n",
       "3  How does the concern about AI bias and its imp...   \n",
       "4  How does the proliferation of AI impact the dy...   \n",
       "\n",
       "                                             context  \\\n",
       "0  And so cognitive dissonance is a powerful forc...   \n",
       "1  And so cognitive dissonance is a powerful forc...   \n",
       "2  So you felt that it was obviously wrong to for...   \n",
       "3  But what scares me is if you plug in AI bias i...   \n",
       "4  - \"you've got a dynamic between the US and Chi...   \n",
       "\n",
       "                                              answer question_type  \\\n",
       "0  The Stoics would advise someone struggling wit...   conditional   \n",
       "1  The Stoics would likely approach the issue of ...   conditional   \n",
       "2  The answer is not explicitly stated in the giv...   conditional   \n",
       "3  The concern about AI bias and its impact on sh...     reasoning   \n",
       "4  The proliferation of AI impacts the dynamic be...        simple   \n",
       "\n",
       "   episode_done  \n",
       "0         False  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = testsetgenerator.generate(docs[:3], test_size=test_size)\n",
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8af2874f-8e13-4951-97bc-66f86e16c4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Should individuals have the right to choose COVID vaccination, considering risks to others, like children?'"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['question'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "2341c854-2f35-46b9-8ee6-863cf69da846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So you felt that it was obviously wrong to force people to get the mRNA vaccine for COVID, right?\n",
      "But if we change a few of the variables, I think your ethical intuitions and certainly political intuitions would totally change.\n",
      "So now we're in an environment where you're deciding not to get vaccinated is putting my kids at risk, right?\n",
      "Do you get to make that choice, right?\n",
      "And you might say, oh, yes, yeah, I should be able to make that choice.\n"
     ]
    }
   ],
   "source": [
    "print(test_df['context'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c201f1c9-dab3-4e22-9174-9042e1707c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df: pd.DataFrame, num: int):\n",
    "    print(df['question'][num])\n",
    "    print(df['context'][num])\n",
    "    print(df['answer'][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "929c05e3-ce21-4468-ba60-5c1cd21f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19666a65-6649-42f3-95d7-f71a0ccb5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llama_index import evaluate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf5b9e-5895-49eb-8da4-6fc196eae627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import download_loader\n",
    "from llama_index import Document\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken # bad ass tokenizer library for use with OpenAI LLMs \n",
    "from llama_index.text_splitter import SentenceSplitter #one of the best on the market\n",
    "\n",
    "#instantiate tokenzier for our embedding model of choice\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#instantiate tokenizer for use with ChatGPT-3.5-Turbo\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912e6cf-0a5b-4982-a883-9ad2bca8afbb",
   "metadata": {},
   "source": [
    "### Converting JSON into single json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a93e1a2-c6b2-47fb-8507-602ba784b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector_search_applications/data/impact_theory_data.json') as f:\n",
    "    data = json.load(f)\n",
    "for file in data[:9]:\n",
    "    filename = file['title'].replace(' ', '_')\n",
    "    for pun in punctuation:\n",
    "        filename = filename.replace(pun, '')\n",
    "    with open(f'./vector_search_applications/practice_data/{filename}.json', 'w') as f:\n",
    "        json.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "dd9d8206-550a-4686-b059-cad49d34e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './vector_search_applications/practice_data/ragas_json_files/'\n",
    "paths = [os.path.join(dir_path, file) for file in os.listdir(dir_path) if file.endswith('json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0cd400c9-2682-430f-869b-68885dd58ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for path in paths:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        unwanted_fields = ['keywords', 'age_restricted', 'description', 'publish_date']\n",
    "        for field in unwanted_fields:\n",
    "            del data[field]\n",
    "    doc = Document(text=data['content'], metadata={k:v for k,v in data.items() if k != 'content'})\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d33d9068-29be-408a-b1b9-790ec83644c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_txt_splitter = SentenceSplitter(chunk_size=256, tokenizer=encoding.encode, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ce5dbdcb-342d-4ebe-a96d-02f0f95fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleNodeParser.from_defaults(text_splitter=gpt35_txt_splitter, include_prev_next_rel=False, include_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "df27fcce-5d13-427e-83dd-9fe6c3e6a31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2994c3f049f04830a92ddf803f6bf13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = parser.get_nodes_from_documents(docs, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e10d38ba-3a8f-4c4f-bad0-7f4332658b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2ca278de-237d-4e5a-89b0-c9b47ec3b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node.metadata = list(node.relationships.values())[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "54b41625-ed98-4dca-a338-61fe425618d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Tom Bilyeu',\n",
       " 'title': 'The Stoic Advice Every Man Learns Too Late In Life | Ryan Holiday',\n",
       " 'video_id': 'gzNLzqI5oTE',\n",
       " 'playlist_id': 'PL8qcvQ7Byc3OJ02hbWJbHWePh4XEg3cvo',\n",
       " 'channel_id': 'UCnYMOamNKLGVlJgRUbamveA',\n",
       " 'length': 10158,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/gzNLzqI5oTE/hq720.jpg',\n",
       " 'views': 120714,\n",
       " 'episode_url': 'https://www.youtube.com/watch?v=gzNLzqI5oTE&list=PL8qcvQ7Byc3OJ02hbWJbHWePh4XEg3cvo'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8bfdc227-2f46-43b2-bc95-ef5ce2c9a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_search_applications.weaviate_interface import WeaviateClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "fcbd1e2d-e7d6-4701-bfd6-6f8abb46df96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read env vars from local .env file\n",
    "api_key = os.environ['WEAVIATE_API_KEY']\n",
    "url = os.environ['WEAVIATE_ENDPOINT']\n",
    "\n",
    "#instantiate client\n",
    "client = WeaviateClient(api_key, url)\n",
    "\n",
    "#check if WCS instance is live and ready\n",
    "client.is_live(), client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1ee66acd-521b-4461-aec2-c532888a0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# construct vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "bcabb68e-0138-471f-8b5e-d76eeac39617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "edadf6a3-0e69-4462-bec0-1b9ac8ac4619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d521256d54166ac3036c07e6127b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = [node.text for node in nodes]\n",
    "embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "0e7025d7-61c8-479b-aca8-a595e767b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, emb in enumerate(embeddings):\n",
    "    nodes[i].embedding = emb.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8049f7e0-51ba-4a82-9fc8-1cdd430c36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    assert isinstance(node.embedding, list)\n",
    "    assert len(node.embedding) == 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f55b2-eecf-4a25-8959-900b04a53b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name ='TestRagas'\n",
    "client.schema.delete_class(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f22190c8-e1f7-43a0-90aa-f6cf3322c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the storage for the embeddings\n",
    "from llama_index.vector_stores.types import VectorStoreQuery as vquery\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"TestRagas\", text_key=\"content\")\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d6fd6-3760-49ad-8466-1ebed796a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.query(query=vquery("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "612cf075-6f79-40fc-b8ff-ce17369052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the index\n",
    "weaviate_index = VectorStoreIndex(nodes, storage_context = storage_context, service_context=service_context, embed_model = minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "59d89dec-27bf-4882-a1f1-5dbbda7f4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.schema.delete_class('TestRagas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "4388c46e-2f45-48a3-893f-94db2497d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df['question'].values.tolist()\n",
    "test_answers = [[item] for item in test_df['answer'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7403e3-2bf2-44c1-a2d5-60871ebe8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext,OpenAIEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "import pandas as pd\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "nest_asyncio.apply()\n",
    "\n",
    "minilm = HuggingFaceEmbeddings(model_name=model_name)\n",
    "def build_query_engine(vector_index, embedding_model):\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=3, embedding_model=model)\n",
    "    return query_engine\n",
    "\n",
    "\"\"\"Async utils.\"\"\"\n",
    "import asyncio\n",
    "from itertools import zip_longest\n",
    "from typing import Any, Coroutine, Iterable, List\n",
    "\n",
    "\n",
    "def run_async_tasks(\n",
    "    tasks: List[Coroutine],\n",
    "    show_progress: bool = False,\n",
    "    progress_bar_desc: str = \"Running async tasks\",\n",
    ") -> List[Any]:\n",
    "    \"\"\"Run a list of async tasks.\"\"\"\n",
    "    tasks_to_execute: List[Any] = tasks\n",
    "    if show_progress:\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            from tqdm.asyncio import tqdm\n",
    "\n",
    "            # jupyter notebooks already have an event loop running\n",
    "            # we need to reuse it instead of creating a new one\n",
    "            nest_asyncio.apply()\n",
    "            loop = asyncio.get_event_loop()\n",
    "\n",
    "            async def _tqdm_gather() -> List[Any]:\n",
    "                return await tqdm.gather(*tasks_to_execute, desc=progress_bar_desc)\n",
    "\n",
    "            tqdm_outputs: List[Any] = loop.run_until_complete(_tqdm_gather())\n",
    "            return tqdm_outputs\n",
    "        # run the operation w/o tqdm on hitting a fatal\n",
    "        # may occur in some environments where tqdm.asyncio\n",
    "        # is not supported\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    async def _gather() -> List[Any]:\n",
    "        return await asyncio.gather(*tasks_to_execute)\n",
    "\n",
    "    outputs: List[Any] = asyncio.run(_gather())\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def chunks(iterable: Iterable, size: int) -> Iterable:\n",
    "    args = [iter(iterable)] * size\n",
    "    return zip_longest(*args, fillvalue=None)\n",
    "\n",
    "\n",
    "async def batch_gather(\n",
    "    tasks: List[Coroutine], batch_size: int = 10, verbose: bool = False\n",
    ") -> List[Any]:\n",
    "    output: List[Any] = []\n",
    "    for task_chunk in chunks(tasks, batch_size):\n",
    "        output_chunk = await asyncio.gather(*task_chunk)\n",
    "        output.extend(output_chunk)\n",
    "        if verbose:\n",
    "            print(f\"Completed {len(output)} out of {len(tasks)} tasks\")\n",
    "    return output\n",
    "\n",
    "async def aquery(query: str, client) -> dict:\n",
    "    response = await client.hybrid_search(query, class_name=index_name, alpha=0.25)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b785a287-29bd-46e3-855d-5fff82684371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d38951af-a00d-4a23-9065-ab8ddc12ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_query_engine = build_query_engine(weaviate_index, minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6b38d118-bd5a-4d49-81c9-d9ee71169c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = evaluate(minilm_query_engine, metrics, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "46201037-6c76-45df-9a0c-78c62fe6eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what are the effects of Aspartame'\n",
    "emb = model.encode(query)\n",
    "bundle = QueryBundle(query_str=query, \n",
    "                     embedding=emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "90f0a961-c9c9-40cb-92d8-9fcbbbe86914",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minilm_query_engine.query(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e5176f3c-0ddc-4071-a25e-0c6ea9566f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "29e2807a-ca33-4fb3-b513-ba5634032da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query: str, class_name: str, client: WeaviateClient):\n",
    "    response = client.hybrid_search(query, class_name, limit=3, alpha=0.25)\n",
    "    return (query, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "6f52b780-ca67-4936-916e-0b0a52fd08fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3768837dd6d74def8156a13b809002d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 41.8 ms, total: 501 ms\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = [get_response(q, index_name, client) for q in tqdm(test_questions)]\n",
    "# progress = tqdm(unit=\"Retriever\", total=len(test_questions))\n",
    "\n",
    "# # with ThreadPoolExecutor(max_workers=os.cpu_count()) as exec:\n",
    "# #     futures = [exec.submit(get_response, query, index_name, client) for query in test_questions]\n",
    "# #     for future in as_completed(futures):\n",
    "# #         results.append(future.result())\n",
    "# #         progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35cc75c-7220-4e42-a2eb-f5f205308598",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prompt_templates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate \u001b[38;5;28;01mas\u001b[39;00m rag_eval\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_answering_prompt\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretriever_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_prompt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT_Turbo\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prompt_templates'"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate as rag_eval\n",
    "from prompt_templates import question_answering_prompt\n",
    "from retriever_pipeline import generate_prompt\n",
    "from openai_interface import GPT_Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a30c880b-4dc0-4efe-afac-1ff1be08b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT_Turbo()\n",
    "context = [d['content'] for d in results[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "ec629602-d760-4784-97a5-16086d9e3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(results: List[dict]):\n",
    "    responses = []\n",
    "    contexts = []\n",
    "    for res in tqdm(results):\n",
    "        context = ''\n",
    "        c = [d['content'] for d in res[1]]\n",
    "        contexts.append(c)\n",
    "        for i, d in enumerate(res[1], 1):\n",
    "            context += f'Context Block: {i}\\n{d[\"content\"]}\\n'\n",
    "            c = d['content']\n",
    "        prompt = question_answering_prompt.format(context=context, question=res[0])\n",
    "        llm_response = gpt.get_completion_from_messages(prompt)\n",
    "        responses.append(llm_response)\n",
    "    return responses, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "4ab1814b-ea15-466c-9a9b-d75178a46815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad450d362368468482eacba4900ea3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers, contexts = get_answers(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d55fa194-785a-4ec5-af56-2762b1e97294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for r in responses:\n",
    "#     answers.append(r.response)\n",
    "#     contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076db4ea-92ca-4f52-a47f-72fc746a0bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca801d1c-96eb-4ae7-84ef-ff2885de7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# ground_truths = test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "73cf07b9-61e7-4e33-a6c0-bcf416adeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truths is not None:\n",
    "    dataset_dict[\"ground_truths\"] = ground_truths\n",
    "ds = Dataset.from_dict(dataset_dict)\n",
    "# result = ragas_evaluate(ds, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "73553f2a-f3b7-455c-a31d-56c6cbbf2a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da21dd433f94db6bcd8469797539b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('./practice_data/ragas_dict.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2018ada9-8973-4217-8aaa-fa799ea2d8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
       "    num_rows: 34\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk('./vector_search_applications/practice_data/ragas_dict.parquet/')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4386dd-8523-4f98-af49-76567cdf36fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-wJ4r3***************************************I17Y. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrag_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/ragas/evaluation.py:105\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, column_map)\u001b[0m\n\u001b[1;32m    103\u001b[0m         binary_metrics\u001b[38;5;241m.\u001b[39mappend(metric\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluating with [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect_columns(metric\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# log the evaluation event\u001b[39;00m\n\u001b[1;32m    108\u001b[0m metrics_names \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics]\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/ragas/metrics/base.py:76\u001b[0m, in \u001b[0;36mMetric.score\u001b[0;34m(self, dataset, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_as_chain_group(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragas_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, callback_manager\u001b[38;5;241m=\u001b[39mcm) \u001b[38;5;28;01mas\u001b[39;00m group:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batches(\u001b[38;5;28mlen\u001b[39m(dataset))):\n\u001b[0;32m---> 76\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m         scores\u001b[38;5;241m.\u001b[39mextend(score)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39madd_column(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, scores)\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/ragas/metrics/context_precision.py:227\u001b[0m, in \u001b[0;36mContextPrecision._score_batch\u001b[0;34m(self, dataset, callbacks, callback_group_name)\u001b[0m\n\u001b[1;32m    224\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mextend(human_prompts)\n\u001b[1;32m    226\u001b[0m responses: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 227\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m responses \u001b[38;5;241m=\u001b[39m [[i\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mgenerations]\n\u001b[1;32m    233\u001b[0m context_lens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(ctx) \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts]\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/ragas/llms/base.py:127\u001b[0m, in \u001b[0;36mLangchainLLM.generate\u001b[0;34m(self, prompts, n, temperature, callbacks)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m=\u001b[39m temperature\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_supports_completions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_multiple_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# call generate_completions n times to mimic multiple completions\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     list_llmresults \u001b[38;5;241m=\u001b[39m run_async_tasks(\n\u001b[1;32m    130\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_completions(prompts, callbacks) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/ragas/llms/base.py:93\u001b[0m, in \u001b[0;36mLangchainLLM.generate_multiple_completions\u001b[0;34m(self, prompts, n, callbacks)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# if BaseChatModel\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     ps \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mformat_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 93\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m old_n\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/base.py:359\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    358\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    363\u001b[0m ]\n\u001b[1;32m    364\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 349\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/base.py:501\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/langchain/chat_models/openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-wJ4r3***************************************I17Y. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "rag_metrics = rag_eval(ds, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da624188-5b81-46c1-93e8-0b65e5792092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
