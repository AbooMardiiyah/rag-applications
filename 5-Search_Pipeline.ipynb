{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6e574f-7a8e-4152-adc3-c3ffdc69cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing import FileIO, Vectorizor\n",
    "from opensearch_interface import OpenSearchClient\n",
    "from reranker import ReRanker\n",
    "from index_templates import youtube_body\n",
    "\n",
    "import os\n",
    "import time\n",
    "from rich import print\n",
    "from dotenv import load_dotenv\n",
    "load_env=load_dotenv('./.env', override=True)\n",
    "from typing import Literal, List\n",
    "from tiktoken_functions import Tokenizer\n",
    "from openai_interface import GPT_Turbo\n",
    "from prompt_templates import question_answering_prompt, question_answering_system, test_prompt\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f06f16-41b3-454b-bb36-29fa5d739c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Answer the following question by reviewing the blocks of context surrounded by triple back ticks:\n",
       "\n",
       "Question:\n",
       "\n",
       "How can one master the art of life?.\n",
       "\n",
       "```So, if you want to be an incredible musician, one of the things you're going to spend a lot of time on are \n",
       "scales. So, you're going to be, once you master that, you master the instrument and the finger movements, I'm \n",
       "assuming one's playing guitar in this analogy, and you master all of that stuff, then you can express yourself, \n",
       "then you can be creative, then you can, as you're saying, you know, create that art. So, when it comes to the art \n",
       "of living, what are the scales? What are the things that people can practice? Obviously, I've read your book, which\n",
       "is tremendous, Green Lights, for anybody that hasn't read it yet, really amazing. Listen to the audiobook. It is \n",
       "unbelievable.\n",
       "\n",
       "It goes from the intellect down into the body, and that's when it becomes an art. That's an individual practice, I \n",
       "think, for everybody. But what we're going to do on the 24th is dive deeper into the sort of the digits, the actual\n",
       "measurable tools of how to get more satisfaction out of life so you can get into the art of living, which is an \n",
       "art, you know, facts and fates. The facts and the science, that's the science of satisfaction. The fate and what \n",
       "the world's doing without our doing, whether our hand's on the wheel or not, where that road goes and how to \n",
       "navigate it, that becomes the art. But the two are not a contradiction. Now, when I think about the great artists \n",
       "and music, especially for somebody living in Austin, seems like a great example.\n",
       "\n",
       "And what do I care about in life? Yes. I was gonna ask you if you're talking to him about that, because when I \n",
       "think about the event that you have coming up, when I think about green lights, when I just think about the concept\n",
       "of the art of living, it's like, you've got three kids, you're gonna have to teach them the art of living. Like, \n",
       "how do you, what is that foundation that you lay for them? Because social media, man, that's, you wanna talk about \n",
       "something that'll mess up the art of living real fast, make you self-conscious in a way that's not useful, that \n",
       "will shape, at that age, oh my God, that will shape the sense of who you are, which then actually impacts who you \n",
       "become. Ooh. It's scary, man. Scary.\n",
       "\n",
       "I love it. The instilling values, what you're doing, what you did with the book, what you're doing now with the Art\n",
       "of Living, the event, which I think is really exciting. If you want to tell people when and where to go for that, \n",
       "it would be amazing. April 24th at <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> a.m. Pacific. Artoflivingevent.com. You can go there and reserve a spot now. \n",
       "It's going to be myself. It's going to be Tony Robbins, Dean Graziosi, Trent Shelton, Mary Ferleo. And we're going \n",
       "to get under the hood of Greenlight's approach and get into the process and hopefully share some tools with you \n",
       "individually that you can apply in your own life. To one, get on the road to the science of the satisfaction you're\n",
       "going to have to then get into the art of living.\n",
       "\n",
       "There are going to be hard times. And I've heard you say something that I think is very powerful, which is never \n",
       "see yourself as a victim. And so as we're all going through this life and things are getting difficult and you're \n",
       "trying to hold on to that image of what you could be, of what life could be, and you're getting lashed by, you \n",
       "know, the reaches of the jungle, but that whether it's religion or just what one ought to do, that you have a very \n",
       "clear vision of what it is on the other side to keep you pushing through all that. Now, you have an event coming up\n",
       "called the Art of Living. Is that what you mean? Oh, that's sure part of it. It's not that you have to be a \n",
       "believer in the art to achieve the art of living.```\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Answer the following question by reviewing the blocks of context surrounded by triple back ticks:\n",
       "\n",
       "Question:\n",
       "\n",
       "How can one master the art of life?.\n",
       "\n",
       "```So, if you want to be an incredible musician, one of the things you're going to spend a lot of time on are \n",
       "scales. So, you're going to be, once you master that, you master the instrument and the finger movements, I'm \n",
       "assuming one's playing guitar in this analogy, and you master all of that stuff, then you can express yourself, \n",
       "then you can be creative, then you can, as you're saying, you know, create that art. So, when it comes to the art \n",
       "of living, what are the scales? What are the things that people can practice? Obviously, I've read your book, which\n",
       "is tremendous, Green Lights, for anybody that hasn't read it yet, really amazing. Listen to the audiobook. It is \n",
       "unbelievable.\n",
       "\n",
       "It goes from the intellect down into the body, and that's when it becomes an art. That's an individual practice, I \n",
       "think, for everybody. But what we're going to do on the 24th is dive deeper into the sort of the digits, the actual\n",
       "measurable tools of how to get more satisfaction out of life so you can get into the art of living, which is an \n",
       "art, you know, facts and fates. The facts and the science, that's the science of satisfaction. The fate and what \n",
       "the world's doing without our doing, whether our hand's on the wheel or not, where that road goes and how to \n",
       "navigate it, that becomes the art. But the two are not a contradiction. Now, when I think about the great artists \n",
       "and music, especially for somebody living in Austin, seems like a great example.\n",
       "\n",
       "And what do I care about in life? Yes. I was gonna ask you if you're talking to him about that, because when I \n",
       "think about the event that you have coming up, when I think about green lights, when I just think about the concept\n",
       "of the art of living, it's like, you've got three kids, you're gonna have to teach them the art of living. Like, \n",
       "how do you, what is that foundation that you lay for them? Because social media, man, that's, you wanna talk about \n",
       "something that'll mess up the art of living real fast, make you self-conscious in a way that's not useful, that \n",
       "will shape, at that age, oh my God, that will shape the sense of who you are, which then actually impacts who you \n",
       "become. Ooh. It's scary, man. Scary.\n",
       "\n",
       "I love it. The instilling values, what you're doing, what you did with the book, what you're doing now with the Art\n",
       "of Living, the event, which I think is really exciting. If you want to tell people when and where to go for that, \n",
       "it would be amazing. April 24th at \u001b[1;36m9\u001b[0m a.m. Pacific. Artoflivingevent.com. You can go there and reserve a spot now. \n",
       "It's going to be myself. It's going to be Tony Robbins, Dean Graziosi, Trent Shelton, Mary Ferleo. And we're going \n",
       "to get under the hood of Greenlight's approach and get into the process and hopefully share some tools with you \n",
       "individually that you can apply in your own life. To one, get on the road to the science of the satisfaction you're\n",
       "going to have to then get into the art of living.\n",
       "\n",
       "There are going to be hard times. And I've heard you say something that I think is very powerful, which is never \n",
       "see yourself as a victim. And so as we're all going through this life and things are getting difficult and you're \n",
       "trying to hold on to that image of what you could be, of what life could be, and you're getting lashed by, you \n",
       "know, the reaches of the jungle, but that whether it's religion or just what one ought to do, that you have a very \n",
       "clear vision of what it is on the other side to keep you pushing through all that. Now, you have an event coming up\n",
       "called the Art of Living. Is that what you mean? Oh, that's sure part of it. It's not that you have to be a \n",
       "believer in the art to achieve the art of living.```\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa3c9cf-a11c-4f6d-9c4d-b9853de4139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index                              uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   kw-impact-theory                   2MjMun4bQYOoeUpv5UsJxg   3   1      33164            0     29.4mb         29.4mb\n",
      "yellow open   semantic-impact-theory-196         SY2nXyvmQ9i5LAS4hmn82g   3   1      37007            0    694.6mb        694.6mb\n",
      "yellow open   kw-impact-theory-196               vsuHausxRb6EjysQriOX5w   3   1      37007            0     30.5mb         30.5mb\n",
      "yellow open   paul-graham3                       -74ZPvxoSMmtCPSzAI9o1A   1   1         18            0    768.2kb        768.2kb\n",
      "yellow open   semantic-impact-theory-128         FJKOre3yT9aFxlF-_TvcTA   3   1      60380            0        1gb            1gb\n",
      "green  open   .opendistro_security               _QeSqO4CQN2IU8VpE9hnPw   1   0         10            0     75.6kb         75.6kb\n",
      "yellow open   semantic-impact-theory             5khyvtPQRASCMmhZiQTcVw   3   1      33164            0    331.5mb        331.5mb\n",
      "yellow open   semantic-impact-theory-evalution   OKatncGsSQipgrq3cRIU0g   1   1      17849            0    348.9mb        348.9mb\n",
      "yellow open   .plugins-ml-config                 IEeXrm-DRiOMm2qzo7PbqA   1   1          1            0      3.9kb          3.9kb\n",
      "green  open   .opensearch-observability          nN299E0QS9OvsRh_UcbJVQ   1   0          0            0       208b           208b\n",
      "yellow open   semantic-impact-theory-gte         HOfyQXRmQLaBYooeLgTcMg   3   1      42863            0      799mb          799mb\n",
      "yellow open   semantic-impact-theory-gte-doc-ids PFdAajMqTM2MB7M2dZcWSg   3   1      17849            0    350.5mb        350.5mb\n",
      "yellow open   kw-impact-theory-evalution         l-vdyoRQSbK4OcnK7ni7cg   1   1      17849            0     29.2mb         29.2mb\n",
      "yellow open   kw-impact-theory-128               a7g5VTfxRBSsQi9hcuJ1NQ   3   1      60380            0     32.7mb         32.7mb\n",
      "yellow open   impact-theory-minilm-196           FzbaKMvCT32zi-YVMdaWUw   3   1      37007            0    367.7mb        367.7mb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osclient=OpenSearchClient()\n",
    "reranker = ReRanker()\n",
    "osclient.show_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8561e3-c042-4d62-869a-82097a8df5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'impact-theory-minilm-196'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c7c5a33b-7c96-44d2-84e6-bc0d9f78e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How can one master the art of life?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "140b1c47-47e6-4fc8-a6d4-48c03cd5f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pipeline(query: str, \n",
    "                      index_name: str,\n",
    "                      search_type: Literal['kw', 'vector', 'hybrid'], \n",
    "                      retriever: OpenSearchClient, \n",
    "                      reranker: ReRanker,\n",
    "                      tokenizer: tiktoken.core.Encoding,\n",
    "                      kw_size: int=50,\n",
    "                      vec_size: int=50,\n",
    "                      top_k: int=4,\n",
    "                      rerank_threshold: float=None,\n",
    "                      token_threshold: int=4000,\n",
    "                      return_text: bool=True,\n",
    "                      verbose: bool=True\n",
    "                      ) -> List[dict]:\n",
    "     \n",
    "    if search_type == 'kw':\n",
    "        results = retriever.keyword_search(query=query, index=index_name, size=kw_size)\n",
    "    elif search_type == 'vector':\n",
    "        results = retriever.vector_search(query=query, index=index_name, size=vec_size)\n",
    "    elif search_type == 'hybrid':\n",
    "        results = retriever.hybrid_search(query=query, \n",
    "                                          kw_index=index_name, \n",
    "                                          vec_index=index_name, \n",
    "                                          kw_size=kw_size,\n",
    "                                          vec_size=vec_size)\n",
    "        \n",
    "    reranked = reranker.rerank(results, query, top_k=top_k, threshold=rerank_threshold)\n",
    "    text = ' '.join([r['_source']['content'] for r in reranked])\n",
    "    token_count = len(tokenizer.encode_batch(text))\n",
    "    if verbose:\n",
    "        print(f'Total Initial Token Count: {token_count}')\n",
    "    if token_count > token_threshold:\n",
    "        print('Token count exceeds token count threshold, reducing size of returned results below token threshold')\n",
    "        while token_count > token_threshold:\n",
    "            num_results = len(reranked)\n",
    "            reranked = reranked[:num_results-1]\n",
    "            text = ' '.join([r['_source']['content'] for r in reranked])\n",
    "            token_count = len(tokenizer.encode_batch(text))\n",
    "        if verbose:\n",
    "            print(f'Total Final Token Count: {token_count}')\n",
    "    if return_text:\n",
    "        return text\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "11f8f49a-76cd-4815-a1bb-e43425e41c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Initial Token Count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3627</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Initial Token Count: \u001b[1;36m3627\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = retrieve_pipeline(query, index_name, 'hybrid', osclient, reranker, tokenizer, top_k=5, return_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf75b58-cad8-4042-b06d-cc69197a8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT_Turbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c542f455-64f4-4b80-a0da-0871cf19f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(base_prompt: str, query: str, results: List[dict]) -> str:\n",
    "    contexts = '\\n\\n'.join([r['content'] for r in results])\n",
    "    prompt = base_prompt.format(question=query, context=contexts)\n",
    "    return prompt\n",
    "    \n",
    "resp = [{'content': '''\n",
    "The potential misuse of ChatGPT and other Large\n",
    "Language Models (LLMs) has raised concerns regarding the\n",
    "dissemination of false information, plagiarism, academic dis-\n",
    "honesty, and fraudulent activities. Consequently, distinguishing\n",
    "between AI-generated and human-generated content has emerged\n",
    "as an intriguing research topic. However, current text detection\n",
    "methods lack precision and are often restricted to specific tasks\n",
    "or domains, making them inadequate for identifying content\n",
    "generated by ChatGPT.\n",
    "In this paper, we propose an effective ChatGPT detector named\n",
    "DEMASQ, which accurately identifies ChatGPT-generated con-\n",
    "tent. Our method addresses two critical factors: (i) the distinct\n",
    "biases in text composition observed in human- and machine-\n",
    "generated content and (ii) the alterations made by humans\n",
    "to evade previous detection methods. DEMASQ is an energy-\n",
    "based detection model that incorporates novel aspects, such\n",
    "as (i) optimization inspired by the Doppler effect to capture\n",
    "the interdependence between input text embeddings and output\n",
    "labels, and (ii) the use of explainable AI techniques to generate\n",
    "diverse perturbations.\n",
    "To evaluate our detector, we create a benchmark dataset\n",
    "comprising a mixture of prompts from both ChatGPT and\n",
    "humans, encompassing domains such as medical, open Q&A,\n",
    "finance, wiki, and Reddit. Our evaluation demonstrates that\n",
    "DEMASQ achieves high accuracy in identifying content gen\n",
    "'''}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322a0675-474e-4305-a20d-a5f7d9df2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Use the below context enclosed in triple back ticks to answer the question. If the context does not provide enough \n",
       "information to answer the question, then use any knowledge you have to answer the question.\n",
       "\n",
       "```\n",
       "The potential misuse of ChatGPT and other Large\n",
       "Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> has raised concerns regarding the\n",
       "dissemination of false information, plagiarism, academic dis-\n",
       "honesty, and fraudulent activities. Consequently, distinguishing\n",
       "between AI-generated and human-generated content has emerged\n",
       "as an intriguing research topic. However, current text detection\n",
       "methods lack precision and are often restricted to specific tasks\n",
       "or domains, making them inadequate for identifying content\n",
       "generated by ChatGPT.\n",
       "In this paper, we propose an effective ChatGPT detector named\n",
       "DEMASQ, which accurately identifies ChatGPT-generated con-\n",
       "tent. Our method addresses two critical factors: <span style=\"font-weight: bold\">(</span>i<span style=\"font-weight: bold\">)</span> the distinct\n",
       "biases in text composition observed in human- and machine-\n",
       "generated content and <span style=\"font-weight: bold\">(</span>ii<span style=\"font-weight: bold\">)</span> the alterations made by humans\n",
       "to evade previous detection methods. DEMASQ is an energy-\n",
       "based detection model that incorporates novel aspects, such\n",
       "as <span style=\"font-weight: bold\">(</span>i<span style=\"font-weight: bold\">)</span> optimization inspired by the Doppler effect to capture\n",
       "the interdependence between input text embeddings and output\n",
       "labels, and <span style=\"font-weight: bold\">(</span>ii<span style=\"font-weight: bold\">)</span> the use of explainable AI techniques to generate\n",
       "diverse perturbations.\n",
       "To evaluate our detector, we create a benchmark dataset\n",
       "comprising a mixture of prompts from both ChatGPT and\n",
       "humans, encompassing domains such as medical, open Q&amp;A,\n",
       "finance, wiki, and Reddit. Our evaluation demonstrates that\n",
       "DEMASQ achieves high accuracy in identifying content gen\n",
       "```\n",
       "\n",
       "Question:\n",
       "\n",
       "What does DEMASQ do?.\n",
       "\n",
       "Answer: \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Use the below context enclosed in triple back ticks to answer the question. If the context does not provide enough \n",
       "information to answer the question, then use any knowledge you have to answer the question.\n",
       "\n",
       "```\n",
       "The potential misuse of ChatGPT and other Large\n",
       "Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m has raised concerns regarding the\n",
       "dissemination of false information, plagiarism, academic dis-\n",
       "honesty, and fraudulent activities. Consequently, distinguishing\n",
       "between AI-generated and human-generated content has emerged\n",
       "as an intriguing research topic. However, current text detection\n",
       "methods lack precision and are often restricted to specific tasks\n",
       "or domains, making them inadequate for identifying content\n",
       "generated by ChatGPT.\n",
       "In this paper, we propose an effective ChatGPT detector named\n",
       "DEMASQ, which accurately identifies ChatGPT-generated con-\n",
       "tent. Our method addresses two critical factors: \u001b[1m(\u001b[0mi\u001b[1m)\u001b[0m the distinct\n",
       "biases in text composition observed in human- and machine-\n",
       "generated content and \u001b[1m(\u001b[0mii\u001b[1m)\u001b[0m the alterations made by humans\n",
       "to evade previous detection methods. DEMASQ is an energy-\n",
       "based detection model that incorporates novel aspects, such\n",
       "as \u001b[1m(\u001b[0mi\u001b[1m)\u001b[0m optimization inspired by the Doppler effect to capture\n",
       "the interdependence between input text embeddings and output\n",
       "labels, and \u001b[1m(\u001b[0mii\u001b[1m)\u001b[0m the use of explainable AI techniques to generate\n",
       "diverse perturbations.\n",
       "To evaluate our detector, we create a benchmark dataset\n",
       "comprising a mixture of prompts from both ChatGPT and\n",
       "humans, encompassing domains such as medical, open Q&A,\n",
       "finance, wiki, and Reddit. Our evaluation demonstrates that\n",
       "DEMASQ achieves high accuracy in identifying content gen\n",
       "```\n",
       "\n",
       "Question:\n",
       "\n",
       "What does DEMASQ do?.\n",
       "\n",
       "Answer: \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What does DEMASQ do?\"\n",
    "prompt = generate_prompt(base_prompt=question_answering_prompt, query=query, results=resp)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a197a4b6-735a-4ca1-b5b1-0e6c7808301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEMASQ is an effective ChatGPT detector that accurately identifies ChatGPT-generated content. It addresses two \n",
       "critical factors: the distinct biases in text composition observed in human- and machine-generated content, and the\n",
       "alterations made by humans to evade previous detection methods. DEMASQ is an energy-based detection model that \n",
       "incorporates novel aspects such as optimization inspired by the Doppler effect to capture the interdependence \n",
       "between input text embeddings and output labels, and the use of explainable AI techniques to generate diverse \n",
       "perturbations.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DEMASQ is an effective ChatGPT detector that accurately identifies ChatGPT-generated content. It addresses two \n",
       "critical factors: the distinct biases in text composition observed in human- and machine-generated content, and the\n",
       "alterations made by humans to evade previous detection methods. DEMASQ is an energy-based detection model that \n",
       "incorporates novel aspects such as optimization inspired by the Doppler effect to capture the interdependence \n",
       "between input text embeddings and output labels, and the use of explainable AI techniques to generate diverse \n",
       "perturbations.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = []\n",
    "\n",
    "resp = gpt.get_completion_from_messages(prompt=prompt, \n",
    "                                        system_message=question_answering_system, \n",
    "                                        max_tokens=250, \n",
    "                                        stream=False,\n",
    "                                        show_response=True)\n",
    "            # # join method to concatenate the elements of the list \n",
    "            # # into a single string, \n",
    "            # # then strip out any empty strings\n",
    "print(resp['choices'][0].message.content)\n",
    "            # # result = \"\".join(report).strip()\n",
    "            # # result = result.replace(\"\\n\", \"\")        \n",
    "            # # res_box.markdown(f'*{result}*') \n",
    "            # print(report[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "135f1c9b-fa2f-41e7-b55c-ea81e3d9a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = '''\n",
    "is mathematically derived for a neural network, “plug and chug” computations can be leveraged to\n",
    "great efficiency to produce more-performant and -generalized models, using very little data.\n",
    "Alongside escalating size and complexity, LLMs are becoming ever more central to applied work\n",
    "in artificial intelligence (AI). Superlative self-attention-based models in natural language processing\n",
    "(NLP) now demonstrate capabilities attracting research interest and investment alongside counter-\n",
    "parts in computer vision, like the diffusion probabilistic models (Ho et al., 2020) in DAll-E (Ramesh\n",
    "et al., 2021) and Stable Diffusion (Rombach et al., 2022). The potential to further amplify capabil-\n",
    "ities by combining text, images, and other modalities to construct even more powerful models, as\n",
    "exemplified by the likes of KOSMOS-1 (Huang et al., 2023) and GPT-4 (OpenAI, 2023), suggests\n",
    "staggering advancements may be on the cusp of development.\n",
    "Still, our collective understanding of the inner workings of these models is far from complete. Lim-\n",
    "ited understanding in the internal mechanisms of models hinders our ability to fully exploit their\n",
    "capabilities, while simultaneously raising challenges (Bommasani et al., 2022). Reliability and\n",
    "safety is a primary concern: LLMs are prone to generating biased and unreliable text, and diffu-\n",
    "sion models produce distorted images that conflict with basic human perception. The unpredictable\n",
    "behaviors of neural models in novel contexts challenges their operational benefits to humans via\n",
    "their (in)abilities to avoid inadvertent harms (Kenton et al., 2021; Weidinger et al., 2021; Tamkin\n",
    "et al., 2021; Hendrycks et al., 2023). Efficiency is also a major concern (Shen et al., 2023)—\n",
    "backpropagation is ubiquituous in optimization, and still entails a high computational cost, particu-\n",
    "larly as models scale over larger amounts of data (Rumelhart et al., 1986a;b), escalating processing\n",
    "requirements.\n",
    "We ask: “how can these challenges can be overcome to ensure models are reliable, interpretable,\n",
    "and efficient?”, and posit that understanding the optimization processes underlying these models is\n",
    "crucial. Perhaps, grasping the intricacies of model optimization will allow for a more straightfor-\n",
    "ward approach, requiring fewer iterations to achieve the same or better quality results? Furthermore,\n",
    "understanding how models optimize allows us to adjust specific parameters in the weight matrices,\n",
    "enabling models to perform in a desired manner. Here, we extend our knowledge of explicit solu-\n",
    "tions from single-layer feed-forward neural networks, to an architecture with compositionally-linked\n",
    "feed-forward and self-attention layers. Our work demonstrates an explicit optimization technique\n",
    "that significantly accelerates model training processes, reaching optima far beyond the reach of\n",
    "backpropagation, alone. So when this solution is applied to self-attention networks, it accelerates\n",
    "time-to-optimization and finds vastly better optima with better generalization qualities, offering a\n",
    "vital alternative to the current trends in neural network training.\n",
    "Explicit solutions relate to recent work focused on finding that attention layers converge in direction\n",
    "to SVM solutions (Tarzanagh et al., 2023) and that transformers may rediscover standard estimation\n",
    "algorithms (Aky¨urek et al., 2023). Explicit solutions also connect to recent discoveries finding gen-\n",
    "eralization in overparametrized networks occurs beyond the point of dataset memorization (Power\n",
    "et al., 2022). Likewise, this work is also connected to efforts aimed at improving the overall train-\n",
    "ing efficiency of transformers, such as one attention type developed to reduce memory reads/writes\n",
    "between GPU high bandwidth memory and on-chip SRAM (Dao et al., 2022).\n",
    "By conducting ablation experiments over a large number of LM architectural variants, we discover\n",
    "that “warming up” (warm-start) models with the explicit solution for self-attention leads to better\n",
    "generalization, more rapidly. This discovery is largely invariant to the scales of training data utilized,\n",
    "i.e., warm-starts lead to objectively better models on both large and small data sets. Furthermore, our\n",
    "findings indicate that iterative optimization with backpropagation only leads to generalized models\n",
    "with the explicit solution—models initialized randomly at least appear to require more computation\n",
    "than any conducted experiments, regardless of scale. We conjecture that model disorientation, in\n",
    "fact, leads to randomly-initialized models not achieving their full potential (regardless of size), and\n",
    "discuss this effect in relation to how LLMs might be overcoming disorientation in applications.\n",
    "2 SAFFU LAYER ARCHITECTURE\n",
    "This derivation began by analyzing word2vec’s continuous bag-of-words (CBOW) variant (Mikolov\n",
    "et al., 2013; Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S., and Dean,\n",
    "Jeff, 2013), and was generalized to simple single-layer LMs, and then all feed-forward neural net-\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e1d7af-8554-4750-b9ed-983fa6dbdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_limit = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d75ae061-ae1c-4468-90c2-5871ed462286",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e410328-44a4-4bc7-9308-40e990d0b642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 256 * 3 * 5\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae2a30f0-8473-4039-90fa-45663662368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "399d00b9-c45d-4df0-887e-1a78ea8d60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(message, input, output):\n",
    "    input_cost = (message + input)/1000 * 0.001\n",
    "    output_cost = output/1000 * 0.002\n",
    "    total_tokens = message + input + output\n",
    "    print(total_tokens)\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01821019-9a03-4cc6-83d3-ee6f8b876ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xd0\\x1a\\xa8\\xc2\\\\\\xaa\\x10\\xc0\\x06oS\\xaf\\xe7\\xaa\\xfdD&i Z\\xae\\xc6\"2O\\xdf\\xfd;DE\\x84\\xe2\\xa7\\xcfD\\x00\\xf4\\xb3,\\xd4\\xdf\\xac\\x85wg\\xc24\\xba\\x93\\xdc\\'\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08', b'\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01']\n",
      "Bad pipe message: %s [b\"J\\x11S\\xbcTu\\xb5a\\x8d\\xbf\\x83\\xc7Q\\xa6\\xec\\x0b\\xd9O\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\"]\n",
      "Bad pipe message: %s [b'\\x92\\x12\\xf7\\xa9\\x83\\x03\\x06\\xc0\\xfc\\x0b\\xe2\\x93e\\x1aY\\xbc\\xe5\\xb3\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3']\n",
      "Bad pipe message: %s [b\"\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00\", b'\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08']\n",
      "Bad pipe message: %s [b'\\x06\\x04\\x01\\x05']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x9d\\xf2T\\x82=\\x9a\\xd0\\xdd\\xd5tP\\x01-\\xcf\\xca\\xc0iW\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x00']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xbb\\xdb\\xd4\\x99\\x94:\\xf1\\xbe\\x18\\x87\\xaf;^\\xce\\x0e\\xcaJ\\xc1\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00', b'\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00']\n",
      "Bad pipe message: %s [b'#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15']\n",
      "Bad pipe message: %s [b'\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xbe\\xc2\\x02cEz\\xc5\\xa7\\xc2\\xf8\\xb3\\x10[\\xefY\"\\xd6p\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0\\'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#']\n",
      "Bad pipe message: %s [b\"\\xecu\\xc6\\xb8\\xa1\\xa2=\\x7f\\x1c\\x02\\x94z5#\\xaf\\xd7\\xe4\\xdf\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\"]\n",
      "Bad pipe message: %s [b\"c\\xaaI1\\x14CK\\xf6?\\xae\\xaf3\\x94%\\xb0\\xa0\\x17\\xc2\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\", b'3\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0']\n",
      "Bad pipe message: %s [b'=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00']\n",
      "Bad pipe message: %s [b\"9\\xcf*H\\xb3\\x0e\\x82\\x99\\x17\\x8b:j\\xc5\\x19\\xaf\\xdd\\x99\\x95\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\", b'\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'l\\x8e}!VX\\xaa\\xcd\\xfd\\xc0\\x14\\xbd4\\xc8s\\xb0 \\xea\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#']\n",
      "Bad pipe message: %s [b'p\\xac\\x08lZD\\r\\x9a5\\xb3qv\\xacS\\x02\\x98\\xb9\\xcf\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/']\n",
      "Bad pipe message: %s [b'Y\\xfcz\\xf1M\\x07C-\\xdb806\\xef$\\xe5k\\xa4\\xfd\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00', b'\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\x9d\\x99\\x1a\\x95\\x07\\x0b\\x845\\xf2\\xc7\\x9a\\\\\\xa58\\t\\xc8\\xbav\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00']\n",
      "Bad pipe message: %s [b'\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08']\n",
      "Bad pipe message: %s [b'\\x96;1\\xb4mZA\\xac\"Wo\\x990\\x12Ko\\x92\\xb7\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0\\'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02', b'\\x04\\x01\\x04\\x02', b'\\x03\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n",
      "Bad pipe message: %s [b'[\\xca1\\x16\\xffW\\xa3/\\xc79\\x7f\\x8cS\\xa7\\x8b\\xa4\\xf51\\x00\\x00']\n",
      "Bad pipe message: %s [b\"0\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\"]\n"
     ]
    }
   ],
   "source": [
    "cost(message, input, output) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31de323-d31d-4b5e-ae6b-f556e40ca7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
