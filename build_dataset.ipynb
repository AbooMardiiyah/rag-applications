{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208b6f8b-09bb-4e2c-bf8b-a5410ceffebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Union, Callable\n",
    "from math import ceil\n",
    "\n",
    "#external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from torch import cuda\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#external files\n",
    "from preprocessing import FileIO\n",
    "import tiktoken # bad ass tokenizer library for use with OpenAI LLMs \n",
    "from llama_index.text_splitter import SentenceSplitter #one of the best on the market\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def chunk_data(data, text_splitter, content_field='content'):\n",
    "    return [text_splitter.split_text(d[content_field]) for d in tqdm(data, 'CHUNKING')]\n",
    "\n",
    "def create_vectors(content_splits: List[List[str]], model: SentenceTransformer):\n",
    "    text_vector_tuples = []\n",
    "    for chunk in tqdm(content_splits, 'VECTORS'):\n",
    "        vectors = model.encode(chunk, show_progress_bar=False, device='cuda:0')\n",
    "        text_vector_tuples.append(list(zip(chunk, vectors)))\n",
    "    return text_vector_tuples\n",
    "\n",
    "def join(corpus, tuples):\n",
    "    docs = []\n",
    "    for i, d in enumerate(corpus):\n",
    "        for j, episode in enumerate(tuples[i]):\n",
    "            doc = {k:v for k,v in d.items() if k != 'content'}\n",
    "            video_id = doc['video_id']\n",
    "            doc['doc_id'] = f'{video_id}_{j}'\n",
    "            doc['content'] = episode[0]\n",
    "            doc['content_embedding'] = episode[1].tolist()\n",
    "            docs.append(doc)\n",
    "    return docs\n",
    "    \n",
    "def create_dataset(corpus: List[dict],\n",
    "                   embedding_model: SentenceTransformer,\n",
    "                   text_splitter: SentenceSplitter,\n",
    "                   file_outpath_prefix: str='./impact-theory-minilmL6',\n",
    "                   content_field: str='content',\n",
    "                   embedding_field: str='content_embedding',\n",
    "                   device: str='cuda:0' if cuda.is_available() else 'cpu'\n",
    "                   ) -> None:\n",
    "    '''\n",
    "    Given a raw corpus of data, this function creates a new dataset where each dataset \n",
    "    doc contains episode metadata and it's associated text chunk and vector representation. \n",
    "    Output is directly saved to disk. \n",
    "    '''\n",
    "    \n",
    "    io = FileIO()\n",
    "\n",
    "    chunk_size = text_splitter.chunk_size\n",
    "    print(f'Creating dataset using chunk_size: {chunk_size}')\n",
    "    start = time.perf_counter()\n",
    "    ########################\n",
    "    # START YOUR CODE HERE #\n",
    "    ########################\n",
    "    content_splits = chunk_data(corpus, text_splitter)\n",
    "    text_vector_tuples = create_vectors(content_splits, embedding_model)\n",
    "    joined_docs = join(corpus, text_vector_tuples)\n",
    "    ########################\n",
    "    # END YOUR CODE HERE #\n",
    "    ########################\n",
    "    file_path = f'{file_outpath_prefix}-{chunk_size}.parquet'\n",
    "    io.save_as_parquet(file_path=file_path, data=joined_docs, overwrite=False)\n",
    "    end = time.perf_counter() - start\n",
    "    print(f'Total Time to process dataset of chunk_size ({chunk_size}): {round(end/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95de892-f23c-4100-9e14-554eef5921cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "#text_splitter\n",
    "splitter = SentenceSplitter(chunk_overlap=0, chunk_size=256, tokenizer=encoding.encode)\n",
    "#model\n",
    "model = SentenceTransformer('/home/elastic/notebooks/vector_search_applications/models/new_ft_model/')\n",
    "#corpus\n",
    "data = FileIO().load_json('./data/impact_theory_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4bb1e48-ad59-4a96-b520-963715ae2692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating dataset using chunk_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating dataset using chunk_size: \u001b[1;36m256\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e71d22edde64436ae60fcf06f8831df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CHUNKING:   0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a5d67a20f9480b98f4a9c4994430bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VECTORS:   0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-30 17:54:37.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msave_as_parquet\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mDataFrame saved as parquet file here: ./impact-theory-newft-256.parquet\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Time to process dataset of chunk_size <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.03</span> minutes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Time to process dataset of chunk_size \u001b[1m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m1.03\u001b[0m minutes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\\\v']\n",
      "Bad pipe message: %s [b'/?\\x1c?\\xf5\\x97q \\xfc\\x97\\xc6\\\\f7 \\xa5\\xf0\\xbc\\xfb\\xf35\\x13\\xbc\\xc8\\xca\\xf3?\\xf9\\xfea\\xecA\\x17.$\\x98\\x9b{\\xd2Q\\xc8M\\x85\\xc6\\xd0q2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17']\n",
      "Bad pipe message: %s [b\"\\x81Qe\\x9f\\xcb\\x8ed\\xb7]\\x95I\\x1e<\\x03g\\xd2\\xc4\\xf8\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00\"]\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00']\n",
      "Bad pipe message: %s [b'\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18']\n",
      "Bad pipe message: %s [b\"G\\x82\\xd8\\x84\\xea\\xdb\\xd2\\x15\\x95\\xe1%\\xba&\\xd3\\x87n\\xd6\\xcb\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\"]\n",
      "Bad pipe message: %s [b'l\\x8b\\xa6\\x1b\\xdc\\x0c\\xaf\\x87s\\xa0_;\\t\\xaex\\x96\\x16\\xdc\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00']\n",
      "Bad pipe message: %s [b'\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01']\n",
      "Bad pipe message: %s [b'', b'\\x00\\x02']\n",
      "Bad pipe message: %s [b'-\\x99u\\xb0\\xb7KW\\xc1}\\x18gt\\x82\\x14\\xea \\xfb\\xc6\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00', b'\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00']\n",
      "Bad pipe message: %s [b'\\x10\\xc0']\n",
      "Bad pipe message: %s [b'\\x15\\xc0\\x0b\\xc0\\x01']\n",
      "Bad pipe message: %s [b'\\xd6\\xb4\\xa7\\xa7\\xf7\\xf1\\xdd\\xe5t\\x95\\xa5V\\xc6\\xf6\\xbf\\xa6f\\xd5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b']\n",
      "Bad pipe message: %s [b'\\x96Az\\x8bG\\x04\\x95\\xb6\\xf4\\xf3\\xc4\\x11\\xe7\\xa8U\\xd4s4\\x00\\x00\\x86\\xc00']\n",
      "Bad pipe message: %s [b\"\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\"]\n",
      "Bad pipe message: %s [b\"L+\\x89&\\xba\\xa9\\xd2'\\xf7\\x17G&\\xf6\\xcfo\\xcbR\\xdc\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\"]\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n"
     ]
    }
   ],
   "source": [
    "create_dataset(data, model, splitter, './impact-theory-newft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c24cb-b761-4675-8c50-4e800f2733c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
