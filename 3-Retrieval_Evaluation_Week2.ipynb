{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d49f27a-15ba-4176-bf6d-211051a8cef6",
   "metadata": {},
   "source": [
    "#### Week 1: Vector Search Applications w/ LLMs.  Authored by Chris Sanchez."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a48d4e-11e4-45b5-8cfc-f595d61e11bd",
   "metadata": {},
   "source": [
    "# Project 1.3\n",
    "\n",
    "# Overview\n",
    "***\n",
    "Welcome to the final notebook for Week 1! Take a look at all the ground we've covered so far:\n",
    "- Chunking/splitting\n",
    "- Vectorization of text\n",
    "- Combining with metadata\n",
    "- Saving to disk\n",
    "- Class Configuration\n",
    "- Data Indexing\n",
    "- Keyword search\n",
    "- Vector search\n",
    "- OPTIONAL: Searching with Filters\n",
    "\n",
    "We are now prepared to move on to a very important topic, **Retrieval Evaluation**.  I hope you've noticed that the search results will differ (sometimes slightly, sometimes by a lot) depeding on which search method you used: `keyword_search` or `vector_search`.  As humans, it's fairly easy for us to determine whether the returned search results are relevant to the query that was submitted, (though even here there will be differing opinions on result relevance).  But how do we systematically determine which search method is better in general?  And how do we measure the relative performance of our retrieval system if we change one of it's parameters...for example, changing our embedding model? What about measuring system performance over time as more documents are added to our datastore?\n",
    "\n",
    "We need a way to evaluate our retrieval system, and this notebook will show you \"one way\" of doing that.  I say \"one way\" because there are many ways to approach this problem, and the method I'm showing you is not perfect (if anything it's a bit too conservative).  Ultimately, measuring retrieval performance is hard because it requires a lot of time and effort, and absent any user [click-data](https://en.wikipedia.org/wiki/Click_tracking), requires some form of data labeling.  With the advent of powerful generative LLMs the process of measuring retrieval performance has become much easier. Let's take a look how that works.\n",
    "\n",
    "# Retrieval Evaluation - Process\n",
    "***\n",
    "Here's a high-level overview of how this Retrieval Evaluation process works:\n",
    "\n",
    "1. Generate a \"golden dataset\" of query-context pairs, from a random selection of documents from the Impact Theory corpus.  I used a pseudo-LlamaIndex implementation for this step.\n",
    "   - **Assumptions**:\n",
    "     - The generated query-context pairs are, in fact, relevant to one another i.e. the query can be answered by the context that it's paired with\n",
    "     - The generated queries are simliar in style and length to the type of queries that end users would ask\n",
    "2. The golden dataset consists of three primary keys: `corpus`, `relevant_docs`, and `queries`\n",
    "     - The `corpus` is the original text context/chunk with it's associated `doc_id`\n",
    "     - The `queries` are the LLM generated queries, one (or more) for each entry in the `corpus`\n",
    "     - The `relevant_docs` is a simple lookup table linking the `corpus` docs to the generated `queries`\n",
    "3. We pass the golden dataset into a retrieval evluation function which does the following:\n",
    "   - Takes in a `retriever` arg (`WeaviateClient`) and a few other configuration params\n",
    "   - Iterates over all queries in the golden dataset and retrieves search results for each query from Weaviate datastore\n",
    "   - Extracts all `doc_id` values from the retrieved results\n",
    "   - Extracts the `doc_id` from the associated `relevant_docs` for each query\n",
    "   - Checks if the relevant doc_id is in the list of retrieved result doc_ids\n",
    "   - After all queries are completed a `hit_rate` score and `mrr` score are calculated for the entire golden dataset\n",
    "   - Writes results to an `eval_results` folder\n",
    "\n",
    "#### In a Nutshell\n",
    "Ulitmately, given a golden dataset consisting of queries, relevant docs, and their associated doc_ids, the `retrieval_evaluation` function is checking if the relevant doc_id is found in the list of retrieved results doc_ids, for each query.\n",
    "\n",
    "#### Problems with this Approach\n",
    "The problems with this approach are many, I'll cover a few here:\n",
    "- The **Assumptions** (see section 1 above) about the golden dataset must hold true.  Given that the pairs are AI generated (I used `gpt-3.5-turbo`), I think the first assumption will generally be true.  When reviewing the dataset I did find a few questions that were not answerable given the context, but for the most part they were.  The 2nd assumption though, is going to be dependent on your particular search use case.  I think for the purposes of this course, the questions generated are a decent reflection of how someone would query this dataset, and therefore do the job of measuring retriever performance.  But I would always check a real-world query distribution before using an approach like the one presented here.\n",
    "- This approach is conversative in that there is only \"one\" right answer.  Either the relevant `doc_id` is in the results list or it isn't.  In reality, there are going to be several documents that could potentially answer the generated query, but we have no way to account for these other relevant documents, unless of course, we want to manually add doc_ids to the golden dataset (and depending on your business case, you may actually want to do that).\n",
    "- We aren't measuring recall or precision because we aren't classifying other documents as \"negatives\".  As was just mentioned, the other documents in the results list may or may not be good matches, we just don't know.  Because we don't know, we can't really classify the other documents as \"negatives\".  So for this approach, we are measuring the [\"hit rate\"](https://uplimit.com/course/vector-search-apps/admin2/content/session_cln9hzpkl00721aah4hbz06fc/module/module_clo3hmyh0006p12cb3bmygky4) which is simply a count of the number of times that we found a relevant `doc_id` match in the results list and [Mean Reciprocal Rank (MRR)](https://uplimit.com/course/vector-search-apps/admin2/content/session_cln9hzpkl00721aah4hbz06fc/module/module_clo3hmyh0006p12cb3bmygky4).  We're using MRR over other metrics such as Mean Average Precision (MAP) because we are only looking at a [single relevant answer](https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank).  Hit rate is a good enough metric for determining if our retriever is retrieving quality results, and MRR will become more important later on when we add a Reranker to the mix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4522b4-5248-4936-af87-b0a2220bdb6d",
   "metadata": {},
   "source": [
    "# Assignment 1.3\n",
    "***\n",
    "#### Instructions:\n",
    "* Import the golden dataset as using the LlamaIndex `EmbeddingQAFinetuneDataset` Class\n",
    "  - **side note: The `EmbeddingQAFinetuneDataset` Class is the same class used for creating fine-tuning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5131c15c-7058-43bb-8cab-763f575c71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#external libraries\n",
    "from llama_index.finetuning import EmbeddingQAFinetuneDataset\n",
    "\n",
    "#standard library imports\n",
    "from typing import List, Tuple \n",
    "import time\n",
    "import os\n",
    "\n",
    "# utilities\n",
    "from tqdm.notebook import tqdm\n",
    "from rich import print\n",
    "from dotenv import load_dotenv\n",
    "env = load_dotenv('./.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bb049-ca24-42ef-9458-72fb687fe27d",
   "metadata": {},
   "source": [
    "### Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e61a946-f5b2-4a7e-80b6-1dc5bdaa4cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset = EmbeddingQAFinetuneDataset.from_json(\"./data/golden_dataset.json\")\n",
    "len(golden_dataset.queries)\n",
    "# list(golden_dataset.queries.values())\n",
    "# golden_dataset.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30095d52-6b57-4632-8a77-08efbec12c80",
   "metadata": {},
   "source": [
    "### Instantiate Weaviate client and set Class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a440bd3e-1223-41c4-ad1a-0120273d9b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = WeaviateClient(os.environ['WEAVIATE_API_KEY'], os.environ['WEAVIATE_ENDPOINT'])\n",
    "class_name = 'Final_test'\n",
    "\n",
    "#check if WCS instance is live and ready\n",
    "client.is_live(), client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e3a263c-1339-4fa9-b9ea-ddec1c45f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.hybrid_search('Why do some people perceive Jordan Peterson as being mean, even though he believes he is trying to help them see the reality of hierarchies and the need for personal growth?', class_name, limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e13a3-9350-40d9-8bfe-0cd37f2ade2f",
   "metadata": {},
   "source": [
    "### Evaluate Retrieval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "255bdabd-770e-4b12-8bd8-54e5cdc64f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate_interface import WeaviateClient\n",
    "from retrieval_evaluation import calc_hit_rate_scores, calc_mrr_scores, record_results\n",
    "\n",
    "def retrieval_evaluation(dataset: EmbeddingQAFinetuneDataset, \n",
    "                         class_name: str, \n",
    "                         retriever: WeaviateClient,\n",
    "                         retrieve_limit: int=5,\n",
    "                         chunk_size: int=256,\n",
    "                         display_properties: List[str]=['doc_id', 'content']\n",
    "                         ) -> Tuple[int, int, int]:\n",
    "\n",
    "    results_dict = {'n':retrieve_limit, \n",
    "                    'Retriever': retriever.model_name_or_path, \n",
    "                    'chunk_size': chunk_size,\n",
    "                    'kw_hit_rate': 0,\n",
    "                    'kw_mrr': 0,\n",
    "                    'vector_hit_rate': 0,\n",
    "                    'vector_mrr': 0,\n",
    "                    'total_misses': 0,\n",
    "                    'total_questions':0\n",
    "                    }\n",
    "        \n",
    "    start = time.perf_counter()\n",
    "    for query_id, q in tqdm(dataset.queries.items(), 'Queries'):\n",
    "        results_dict['total_questions'] += 1\n",
    "        \n",
    "        #make Keyword, Vector, and Hybrid calls to Weaviate host\n",
    "        try:\n",
    "            kw_response = retriever.keyword_search(request=q, class_name=class_name, limit=retrieve_limit, display_properties=display_properties)\n",
    "            vector_response = retriever.vector_search(request=q, class_name=class_name, limit=retrieve_limit, display_properties=display_properties)\n",
    "            \n",
    "            #collect doc_ids to check for document matches (include only results_top_k)\n",
    "            kw_doc_ids = {result['doc_id']:i for i, result in enumerate(kw_response, 1)}\n",
    "            vector_doc_ids = {result['doc_id']:i for i, result in enumerate(vector_response, 1)}\n",
    "            \n",
    "            #extract doc_id for scoring purposes\n",
    "            doc_id = dataset.relevant_docs[query_id][0]\n",
    "     \n",
    "            #increment hit_rate counters and mrr scores\n",
    "            if doc_id in kw_doc_ids:\n",
    "                results_dict['kw_hit_rate'] += 1\n",
    "                results_dict['kw_mrr'] += 1/kw_doc_ids[doc_id]\n",
    "            if doc_id in vector_doc_ids:\n",
    "                results_dict['vector_hit_rate'] += 1\n",
    "                results_dict['vector_mrr'] += 1/vector_doc_ids[doc_id]\n",
    "\n",
    "            # if no hits, let's capture that\n",
    "            else:\n",
    "                # print(kw_response)\n",
    "                results_dict['total_misses'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    print(results_dict['kw_hit_rate'])\n",
    "    print(results_dict['vector_hit_rate'])\n",
    "    \n",
    "    #use raw counts to calculate final scores\n",
    "    calc_hit_rate_scores(results_dict)\n",
    "    calc_mrr_scores(results_dict)\n",
    "    \n",
    "    end = time.perf_counter() - start\n",
    "    print(f'Total Processing Time: {round(end/60, 2)} minutes')\n",
    "    record_results(results_dict, chunk_size, './eval_results', as_text=True)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5ed71-d150-4312-9d41-0b43ae43bd72",
   "metadata": {},
   "source": [
    "### Run evaluation over golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa6c2a69-2f17-4b06-b046-4504ea953032",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_100 = EmbeddingQAFinetuneDataset.from_json('./data/golden_100.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d42568cd-d0e3-4640-8865-37b6dff80307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a3b1697f8d41a09a3eb90a886e2f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m72\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m37\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Processing Time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.39</span> minutes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Processing Time: \u001b[1;36m0.39\u001b[0m minutes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n': 5,\n",
       " 'Retriever': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'chunk_size': 256,\n",
       " 'kw_hit_rate': 0.72,\n",
       " 'kw_mrr': 0.6,\n",
       " 'vector_hit_rate': 0.37,\n",
       " 'vector_mrr': 0.29,\n",
       " 'total_misses': 63,\n",
       " 'total_questions': 100}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_evaluation(golden_100, class_name, client, retrieve_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a07887bc-e074-417f-afcf-e1aa98fb8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n': 10,\n",
    " 'top_k': 5,\n",
    " 'alpha': 0.3,\n",
    " 'Retriever': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    " 'Ranker': 'None',\n",
    " 'chunk_size': 256,\n",
    " 'kw_hit_rate': 0.68,\n",
    " 'kw_mrr': 0.55,\n",
    " 'vector_hit_rate': 0.4,\n",
    " 'vector_mrr': 0.29,\n",
    " 'hybrid_hit_rate': 0.71,\n",
    " 'hybrid_mrr': 0.56,\n",
    " 'total_misses': 73,\n",
    " 'total_questions': 250}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8235f53-979e-4232-83ef-3f4e57829b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
