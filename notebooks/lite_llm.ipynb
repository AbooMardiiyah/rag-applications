{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0483dc-785e-4286-9870-91567bc1ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa8105b-32f0-4686-8df5-54542797602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "envs = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from litellm import batch_completion\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import get_token_count, load_azure_openai\n",
    "from src.llm.prompt_templates import huberman_system_prompt\n",
    "from app_features import generate_prompt_series\n",
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "from tiktoken import Encoding\n",
    "from rich import print\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3103fa2-f5b8-4036-bfef-56994e1e1acb",
   "metadata": {},
   "source": [
    "### Set Constants\n",
    "---\n",
    "\n",
    "### Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e325cb21-c86c-4404-80d8-3b39af8ef9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/openai/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Huberman_subset_minilm_test'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Huberman_minilm_128'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Huberman_minilm_256'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Huberman_minilm_512'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Huberman_subset_minilm_test'\u001b[0m, \u001b[32m'Huberman_minilm_128'\u001b[0m, \u001b[32m'Huberman_minilm_256'\u001b[0m, \u001b[32m'Huberman_minilm_512'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get weaviate client\n",
    "\n",
    "weave_client = get_weaviate_client()\n",
    "collections = weave_client.show_all_collections()\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2d106-0f96-42fb-a00d-a5a493b3d957",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad60d040-3e93-41e6-b489-86529617fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo_model = \"gpt-3.5-turbo-0125\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "azure_model = \"gpt-35-turbo\" #\"gpt-4-32k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7218f9fa-f07a-4875-b55f-248cb8e28ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = LLM(model_name=claude_model, api_key=os.environ['ANTHROPIC_API_KEY'])\n",
    "turbo = LLM(model_name=turbo_model)\n",
    "azure = load_azure_openai(model_name=azure_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f4064-5fbf-4e88-adc2-40d9a8696cf8",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118b99e7-ebed-49be-bb16-aa32f6238663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/openai/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "query = 'What does Cal Newport have to say about avoiding distractions'\n",
    "\n",
    "results = weave_client.hybrid_search(request=query,\n",
    "                                     collection_name=collections[2],\n",
    "                                     return_properties=['content', 'title', 'summary','guest'],\n",
    "                                     limit=5\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd3281-c3c9-4c62-a58b-9d6cff687136",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcba9a1b-d11d-4c9a-9b52-558d79f9a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rough Total Token Count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3323</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rough Total Token Count: \u001b[1;36m3323\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant_message = generate_prompt_series(query, results[:5])\n",
    "max_tokens = 500\n",
    "token_count = get_token_count(assistant_message) + get_token_count(huberman_system_prompt) + max_tokens\n",
    "print(f'Rough Total Token Count: {token_count}')\n",
    "# print(assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a6d09-7e88-461a-82c3-448f90011d92",
   "metadata": {},
   "source": [
    "### LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44271ee1-99f6-428b-aa22-1059103c9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 13.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_count = 0\n",
    "\n",
    "async def gather_tasks():\n",
    "    tasks = [azure.achat_completion( system_message=huberman_system_prompt,\n",
    "                                       assistant_message=assistant_message,\n",
    "                                       temperature=1.0,\n",
    "                                       max_tokens=max_tokens,\n",
    "                                       raw_response=True\n",
    "                                       ) for x in range(3)]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses\n",
    "\n",
    "# for i in range(1,16):\n",
    "#     completion = await \n",
    "#     tokens = completion.usage.total_tokens\n",
    "#     token_count += tokens\n",
    "#     print(f'{i}.) Running Token Count: {token_count}')\n",
    "#     print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb88bb89-a4b9-43cb-b543-ea0d58151967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.47</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1.47\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "responses = asyncio.run(gather_tasks())\n",
    "end = time.perf_counter() - start\n",
    "print(f'{round(end,2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "afdf4b8a-1e09-440e-a203-8be2f23875c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 ms, sys: 4.01 ms, total: 44.9 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "message, cost = azure.chat_completion(huberman_system_prompt, assistant_message, temperature=1.0, return_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "241a73d9-d6a9-44bd-a1ff-e96ed6d6c258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00215"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2800/1000 * 0.0005 + (500/1000*0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "87a6fd95-5285-47c5-b2ca-9e7952e15a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004451</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0.004451\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cal Newport emphasizes the importance of eliminating distractions, such as social media, smartphones, and excessive\n",
       "emails, in order to optimize cognitive performance. He also suggests creating specific protocols for different \n",
       "types of tasks and using specialized tools, like whiteboards and notebooks, to maximize focus and efficiency. \n",
       "Additionally, Newport cautions against the addictive nature of smartphones and social media, and suggests that \n",
       "individuals reconsider unrestricted internet usage, particularly among young people. Therefore, Cal Newport \n",
       "highlights the significance of avoiding distractions to enhance focus and productivity.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cal Newport emphasizes the importance of eliminating distractions, such as social media, smartphones, and excessive\n",
       "emails, in order to optimize cognitive performance. He also suggests creating specific protocols for different \n",
       "types of tasks and using specialized tools, like whiteboards and notebooks, to maximize focus and efficiency. \n",
       "Additionally, Newport cautions against the addictive nature of smartphones and social media, and suggests that \n",
       "individuals reconsider unrestricted internet usage, particularly among young people. Therefore, Cal Newport \n",
       "highlights the significance of avoiding distractions to enhance focus and productivity.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9af68ab-58ca-462e-9cc3-5ae0fb1748e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_task(message: list[dict]):\n",
    "    response = await acompletion(model=\"gpt-3.5-turbo-1106\", messages=message, temperature=1.0)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaa73ef4-d58d-45d8-bd9f-3802a7b284c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "async def gather(prompts: list[str]):\n",
    "    tasks = []\n",
    "    for p in prompts:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a highly experienced data annotator.  Your job is to create two questions that can be answered from the provided context.\"},\n",
    "                    {\"role\": \"assistant\", \"content\": prompt.format(context=p)}]\n",
    "        tasks.append(async_task(messages))\n",
    "    asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bb1ff5e-be9f-4dfc-9433-3e95c7e3400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 µs, sys: 109 µs, total: 437 µs\n",
      "Wall time: 416 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "responses = asyncio.run(gather(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08509474-ba0b-4c53-bc1e-6ab466a8949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35e45c-caf0-49cb-911f-922c1c2816c8",
   "metadata": {},
   "source": [
    "### Mulitple LLM calls single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79d6e2f-f609-4dce-9f4b-de69f9044eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What does Cal Newport have to say about avoiding distractions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41afdb55-1aef-47fc-8fce-0223c9b8c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate {n}\n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines.\\n\\nOriginal question: {question}\n",
    "\"\"\".format(n=3, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f4827-4015-4a56-ad5b-1181c4672b9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "litellm.main.completion() got multiple values for keyword argument 'api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mazure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_API_VERSION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebooks/vectorsearch-applications/notebooks/../src/llm/llm_interface.py:71\u001b[0m, in \u001b[0;36mLLM.chat_completion\u001b[0;34m(self, system_message, assistant_message, temperature, max_tokens, stream, raw_response, return_cost, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m temperature\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     66\u001b[0m messages \u001b[38;5;241m=\u001b[39m  [\n\u001b[1;32m     67\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: initial_role, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: system_message},\n\u001b[1;32m     68\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: assistant_message}\n\u001b[1;32m     69\u001b[0m             ]\n\u001b[0;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m completion(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     72\u001b[0m                     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     73\u001b[0m                     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     74\u001b[0m                     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m     75\u001b[0m                     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m     76\u001b[0m                     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key,\n\u001b[1;32m     77\u001b[0m                     api_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base,\n\u001b[1;32m     78\u001b[0m                     api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_version,\n\u001b[1;32m     79\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m cost \u001b[38;5;241m=\u001b[39m completion_cost(response, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, messages\u001b[38;5;241m=\u001b[39mmessages,call_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_response:\n",
      "\u001b[0;31mTypeError\u001b[0m: litellm.main.completion() got multiple values for keyword argument 'api_key'"
     ]
    }
   ],
   "source": [
    "response = azure.chat_completion(system_msg, temperature=1.0, raw_response=False,  api_key=os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                      api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "                      api_base=os.environ['AZURE_OPENAI_ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "337fcb4e-c761-4a73-a052-f697a357b093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. How does Cal Newport discuss strategies for minimizing distractions?\\n2. What are Cal Newport's recommendations for staying focused and avoiding distractions?\\n3. What insights does Cal Newport provide on the topic of mitigating distractions and maintaining concentration?\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03a99caa-f82b-4368-be43-f82f91f8cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does Cal Newport discuss strategies for minimizing distractions?',\n",
       " \"What are Cal Newport's recommendations for staying focused and avoiding distractions?\",\n",
       " 'What insights does Cal Newport provide on the topic of mitigating distractions and maintaining concentration?',\n",
       " 'What does Cal Newport have to say about avoiding distractions']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\n",
    "                re.sub(r\"^[-\\d]+[\\).\\s]\", \"\", question).strip() for question in response.split('\\n')\n",
    "            ]\n",
    "questions.append(query)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "31612c5e-734c-4233-b943-cd4b40f9c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = batch_completion(model=f'azure/{azure_model}', messages=messages, temperature=1.0, max_tokens=500,  api_key=os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                      api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "                      api_base=os.environ['AZURE_OPENAI_ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80681a4d-c886-49e1-b80d-8befe9c58480",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals = [weave_client.hybrid_search(request=query,\n",
    "                                     collection_name=collections[2],\n",
    "                                     return_properties=['content', 'title', 'summary','guest'],\n",
    "                                     limit=3\n",
    "                                    ) for query in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8c4d66f9-fda4-4e50-b85f-d5be67ce8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_messages = [generate_prompt_series(q, retrievals[i]) for i, q in enumerate(questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43ad16e6-a21f-46d9-b3b7-165b0944366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ [{'role':'system','content':huberman_system_prompt},\n",
    "               {'role':'assistant', 'content': mess}] for mess in assist_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c045f4c1-0a34-4cf5-9b4e-15a8305b1a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cal Newport discusses strategies for minimizing distractions by emphasizing the importance of eliminating \n",
       "distractions such as social media, smartphones, and excessive emails in order to optimize cognitive performance. He\n",
       "also suggests creating specific protocols for different types of tasks and using specialized tools like whiteboards\n",
       "and notebooks to maximize focus and efficiency. Additionally, Newport cautions against the addictive nature of \n",
       "smartphones and social media, and suggests that individuals reconsider unrestricted internet usage, particularly \n",
       "among young people. By prioritizing deep work and minimizing distractions, Newport asserts that individuals can \n",
       "improve their cognitive performance and achieve their best possible work.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cal Newport discusses strategies for minimizing distractions by emphasizing the importance of eliminating \n",
       "distractions such as social media, smartphones, and excessive emails in order to optimize cognitive performance. He\n",
       "also suggests creating specific protocols for different types of tasks and using specialized tools like whiteboards\n",
       "and notebooks to maximize focus and efficiency. Additionally, Newport cautions against the addictive nature of \n",
       "smartphones and social media, and suggests that individuals reconsider unrestricted internet usage, particularly \n",
       "among young people. By prioritizing deep work and minimizing distractions, Newport asserts that individuals can \n",
       "improve their cognitive performance and achieve their best possible work.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cal Newport recommends several strategies for staying focused and avoiding distractions. Some of his \n",
       "recommendations include: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Eliminating distractions such as social media, smartphones, and excessive emails to optimize cognitive \n",
       "performance.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Creating specific protocols for different types of tasks and using specialized tools like whiteboards and \n",
       "notebooks to maximize focus and efficiency.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Emphasizing the concept of active recall, which involves replicating and recalling information to improve \n",
       "learning.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Prioritizing deep work and minimizing distractions to improve cognitive performance.\n",
       "\n",
       "These recommendations are based on the principles outlined in Newport's book <span style=\"color: #008000; text-decoration-color: #008000\">\"Deep Work: Rules for Focus Success in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a Distracted World\"</span> and align with his expertise in productivity and focus.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cal Newport recommends several strategies for staying focused and avoiding distractions. Some of his \n",
       "recommendations include: \n",
       "\u001b[1;36m1\u001b[0m. Eliminating distractions such as social media, smartphones, and excessive emails to optimize cognitive \n",
       "performance.\n",
       "\u001b[1;36m2\u001b[0m. Creating specific protocols for different types of tasks and using specialized tools like whiteboards and \n",
       "notebooks to maximize focus and efficiency.\n",
       "\u001b[1;36m3\u001b[0m. Emphasizing the concept of active recall, which involves replicating and recalling information to improve \n",
       "learning.\n",
       "\u001b[1;36m4\u001b[0m. Prioritizing deep work and minimizing distractions to improve cognitive performance.\n",
       "\n",
       "These recommendations are based on the principles outlined in Newport's book \u001b[32m\"Deep Work: Rules for Focus Success in\u001b[0m\n",
       "\u001b[32ma Distracted World\"\u001b[0m and align with his expertise in productivity and focus.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cal Newport provides insights on the importance of eliminating distractions, such as social media, smartphones, and\n",
       "excessive emails, in order to optimize cognitive performance. He suggests creating specific protocols for different\n",
       "types of tasks and using specialized tools, like whiteboards and notebooks, to maximize focus and efficiency. \n",
       "Newport also cautions against the addictive nature of smartphones and social media, and suggests that individuals \n",
       "reconsider unrestricted internet usage, particularly among young people. By prioritizing deep work and minimizing \n",
       "distractions, Newport asserts that individuals can improve their cognitive performance and achieve their best \n",
       "possible work. This shows that Newport emphasizes the significance of mitigating distractions and maintaining \n",
       "concentration for optimal cognitive performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cal Newport provides insights on the importance of eliminating distractions, such as social media, smartphones, and\n",
       "excessive emails, in order to optimize cognitive performance. He suggests creating specific protocols for different\n",
       "types of tasks and using specialized tools, like whiteboards and notebooks, to maximize focus and efficiency. \n",
       "Newport also cautions against the addictive nature of smartphones and social media, and suggests that individuals \n",
       "reconsider unrestricted internet usage, particularly among young people. By prioritizing deep work and minimizing \n",
       "distractions, Newport asserts that individuals can improve their cognitive performance and achieve their best \n",
       "possible work. This shows that Newport emphasizes the significance of mitigating distractions and maintaining \n",
       "concentration for optimal cognitive performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cal Newport emphasizes the importance of avoiding distractions such as social media, smartphones, and excessive \n",
       "emails in order to optimize cognitive performance. He suggests creating specific protocols for different types of \n",
       "tasks and using specialized tools like whiteboards and notebooks to maximize focus and efficiency. Newport also \n",
       "highlights the concept of active recall, which involves replicating and recalling information to improve learning. \n",
       "Additionally, Newport is structured and disciplined in his avoidance of cell phone use, as mentioned by Andrew \n",
       "Huberman in the podcast transcript. Therefore, Cal Newport advocates for minimizing distractions and maintaining a \n",
       "structured approach to enhance focus and productivity.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cal Newport emphasizes the importance of avoiding distractions such as social media, smartphones, and excessive \n",
       "emails in order to optimize cognitive performance. He suggests creating specific protocols for different types of \n",
       "tasks and using specialized tools like whiteboards and notebooks to maximize focus and efficiency. Newport also \n",
       "highlights the concept of active recall, which involves replicating and recalling information to improve learning. \n",
       "Additionally, Newport is structured and disciplined in his avoidance of cell phone use, as mentioned by Andrew \n",
       "Huberman in the podcast transcript. Therefore, Cal Newport advocates for minimizing distractions and maintaining a \n",
       "structured approach to enhance focus and productivity.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for response in responses:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34810d27-e423-426d-90a1-b2970eed8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_messages = [{'role':'system','content':huberman_system_prompt},\n",
    "                   {'role':'assistant', 'content': assistant_message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720859bf-06ee-4b46-9ae5-cf636cf0ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere = LLM(model_name='command-r-plus', api_key=os.environ['COHERE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf1ed69-55dd-4d23-9564-11a325776eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cal Newport emphasizes the importance of eliminating distractions to optimize cognitive performance and enhance focus and productivity. He suggests that individuals should minimize their engagement with distractions such as social media, smartphones, and excessive emails. Newport also cautions against the addictive nature of these technologies, particularly for young people, and recommends reconsidering unrestricted internet usage. By prioritizing deep work and creating specific protocols for different tasks, individuals can improve their ability to focus and perform at their cognitive best.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere.chat_completion(system_message=huberman_system_prompt,\n",
    "                       assistant_message=assistant_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71dfc6-3294-4a5e-a63d-83005e312c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
