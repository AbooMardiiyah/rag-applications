{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa8105b-32f0-4686-8df5-54542797602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "envs = load_dotenv('../.env', override=True)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84acac16-e95c-4457-9b34-48f46ab52edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from litellm import completion_with_retries, completion, acompletion\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from openai import AsyncOpenAI\n",
    "from time import sleep\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325cb21-c86c-4404-80d8-3b39af8ef9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edc2756-b14a-46b2-b320-607bc353b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(name='anthro', \n",
    "                    model_name=\"claude-3-sonnet-20240229\",\n",
    "                    temperature=1.0, \n",
    "                    anthropic_api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf9556f-45fa-4774-ab41-7d75ceb6270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"how can langsmith help with testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7239fd0a-80f1-4170-aa3a-79c638409ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUESTION=\"What show won the Outstanding Drama award at the 2024 Emmys?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff014894-a650-4cea-8146-639af4076e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_QUERIES=f\"\"\"Human: You are an expert at generating search queries for the Brave search engine.\n",
    "Generate three search queries that are relevant to this question. Output only valid JSON.\n",
    "\n",
    "User question: {USER_QUESTION}\n",
    "\n",
    "Format: {{\"queries\": [\"query_1\", \"query_2\", \"query_3\"]}}\\n\\nAssistant: {{\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f36a64e-68d4-4240-b0f2-029b2004da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an expert at generating search queries for the Brave search engine.\n",
      "Generate three search queries that are relevant to this question. Output only valid JSON.\n",
      "\n",
      "User question: What show won the Outstanding Drama award at the 2024 Emmys?\n",
      "\n",
      "Format: {\"queries\": [\"query_1\", \"query_2\", \"query_3\"]}\n",
      "\n",
      "Assistant: {\n"
     ]
    }
   ],
   "source": [
    "print(GENERATE_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f040f7e2-7ec9-4c04-b4c4-cb60ca6b813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa5732e-f2a9-46fb-bdc7-d87bd3137ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "                {\"role\": \"user\", \"content\": GENERATE_QUERIES},\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b260e229-53fe-4a93-918b-04dd077d56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.messages.create(model=\"claude-3-haiku-20240307\", \n",
    "                                    temperature=1.0, \n",
    "                                    max_tokens=500,\n",
    "                                    messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d00b194-bc99-4b9f-afff-187894773f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.usage.input_tokens, message.usage.output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "397165cd-474c-4723-821a-d2bbe00d4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_message = completion(model=\"claude-3-haiku-20240307\",\n",
    "           messages=messages,\n",
    "           temperature=1.0,\n",
    "           max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7bfc66e-8572-4840-bdf0-9349d5bf819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-bafd81ac-8579-437d-b030-ecff4a1ed14b',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'message': {'content': '\"queries\": [\\n  \"2024 Emmy awards outstanding drama winner\",\\n  \"top drama series emmy 2024\",\\n  \"most awarded drama series 2024 emmys\"\\n]\\n\\n}',\n",
       "    'role': 'assistant'}}],\n",
       " 'created': 1712030502,\n",
       " 'model': 'claude-3-haiku-20240307',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'prompt_tokens': 88, 'completion_tokens': 49, 'total_tokens': 137}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lite_message.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66897f6f-2cb5-4dea-8b54-5f51c9f6df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "context1 = '''\n",
    "Another benefit of gRPC is its enhanced type safety, achieved through the explicit definition of data structures and types in protobufs. This approach significantly reduces common challenges encountered when working with GraphQL and JSON, such as ambiguity in data types and difficulties in ensuring readability.\n",
    "In the context of JSON and GraphQL, determining the correct data type for numbers (e.g., distinguishing between integers and floating-point numbers) or interpreting empty properties can be problematic. In Weaviate, this can lead to AutoSchema inferring inappropriate data types, potentially leading to data integrity issues.\n",
    "Moreover, parsing complex and deeply nested JSON responses, or crafting intricate GraphQL queries, can degrade the developer experience and elevate the risk of errors. This is also partly due to having to conform to the structure of the GraphQL schema. This has been a challenge for certain tasks, such as implementing GroupBy queries and responses in Weaviate.\n",
    "gRPC addresses these issues head-on by facilitating the customization of data structures for both requests and responses. Its reliance on strictly defined data types streamlines parsing processes and boosts reliability.\n",
    "'''\n",
    "\n",
    "context2 = '''\n",
    "Vectors are a great way to represent meaning. Vectors are arrays of elements that can capture meaning from different data types, such as texts, images, videos, and other content. The elements are called dimensions. High dimension vectors capture more information, but they are harder to work with.\n",
    "Vector databases make it easier to work with high dimensional vectors. Consider search; Vector databases efficiently measure semantic similarity between data objects. When you run a similarity search, a vector database like Weaviate uses a vectorized version of the query to find objects in the database that have vectors similar to the query vector.\n",
    "Vectors are like coordinates in a multi-dimensional space. A very simple vector might represent objects, words in this case, in a 2-dimensional space.\n",
    "In the graph below, the words Apple and Banana are shown close to each other. Newspaper and Magazine are also close to each other, but they are far away from Apple and Banana in the same vector space.\n",
    "Within each pair, the distance between words is small because the objects have similar vector representations. The distance between the pairs is larger because the difference between the vectors is larger. Intuitively, fruits are similar to each other, but fruits are not similar to reading material.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1acda5a8-2af8-4ec8-b9a3-ef32364047e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [context1, context2]\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a highly experienced data annotator.  Your job is to create two questions that can be answered from the provided context.\"},\n",
    "            {\"role\": \"assistant\", \"content\": context}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7c33819-6da0-4026-953d-500fd51a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cage = []\n",
    "# for chunk in completion(model='claude-3-haiku-20240307', messages=messages, temperature=1.0, stream=True):\n",
    "#     cage.append(chunk['choices'][0]['delta']['content'].strip())\n",
    "#     if any(cage):\n",
    "#         print(' '.join(cage))\n",
    "#         sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9af68ab-58ca-462e-9cc3-5ae0fb1748e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_task(message: list[dict]):\n",
    "    response = await acompletion(model=\"gpt-3.5-turbo-1106\", messages=message, temperature=1.0)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaa73ef4-d58d-45d8-bd9f-3802a7b284c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "async def gather(prompts: list[str]):\n",
    "    tasks = []\n",
    "    for p in prompts:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a highly experienced data annotator.  Your job is to create two questions that can be answered from the provided context.\"},\n",
    "                    {\"role\": \"assistant\", \"content\": prompt.format(context=p)}]\n",
    "        tasks.append(async_task(messages))\n",
    "    asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bb1ff5e-be9f-4dfc-9433-3e95c7e3400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 µs, sys: 109 µs, total: 437 µs\n",
      "Wall time: 416 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "responses = asyncio.run(gather(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08509474-ba0b-4c53-bc1e-6ab466a8949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ac28e-0773-400e-8192-74f7d471da6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
