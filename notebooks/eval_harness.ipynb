{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d7caf1-454d-4df6-a561-8169397fe6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d44c3272-54d4-4b6e-be25-1f8f76ac7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from evaluation.custom_eval_models import (CustomAzureOpenAI, AnswerCorrectnessMetric, \n",
    "                                            EvalResponse, TestCaseBundle)\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from src.reranker import ReRanker\n",
    "from src.llm.prompt_templates import context_block\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import load_azure_openai\n",
    "from src.llm.prompt_templates import huberman_system_message, question_answering_prompt_series\n",
    "from app_features import generate_prompt_series\n",
    "\n",
    "from loguru import logger\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda2020-5a6b-4514-86a6-880afa9cd8b4",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131066a0-9471-4199-a683-954fd4a773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/golden_datasets/golden_256.json'\n",
    "data = FileIO().load_json(data_path)\n",
    "queries = list(data['queries'].values())\n",
    "\n",
    "#get random set of questions for eavl\n",
    "random_questions = sample(queries, k=25)\n",
    "assert len(random_questions) == len(set(random_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef34f1-e110-40ea-8789-b77859f5ea1c",
   "metadata": {},
   "source": [
    "### Set System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc46f6f8-4413-40f5-934d-a7564a64b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/openai/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "client = get_weaviate_client()\n",
    "collection_name = 'Huberman_minilm_256'\n",
    "reranker= ReRanker()\n",
    "llm = load_azure_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21e181-8d66-4351-99cd-d50bc62ea2e2",
   "metadata": {},
   "source": [
    "### Create Retrieval Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f2c45d-029a-41a9-9a38-3757fa035b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_blocks(results: list[dict],\n",
    "                          summary_key: str='summary',\n",
    "                          guest_key: str='guest',\n",
    "                          content_key: str='content'):\n",
    "    context_series = [context_block.format(summary=res[summary_key],\n",
    "                                          guest=res[guest_key],\n",
    "                                          transcript=res[content_key]) \n",
    "                      for res in results]\n",
    "    return context_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6ba4a-899b-4676-817a-17395664ab6c",
   "metadata": {},
   "source": [
    "### Set Eval Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc5bb67-fd2c-4530-9ba1-109023fac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_eval_model = CustomAzureOpenAI('gpt-4')\n",
    "acm = AnswerCorrectnessMetric(model=azure_eval_model, strict=False).get_metric()\n",
    "faith = FaithfulnessMetric(model=azure_eval_model)\n",
    "# metrics = [AnswerCorrectnessMetric(model=azure_eval_model, strict=False).get_metric(), FaithfulnessMetric(threshold=0.7, model=azure_eval_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ce50d4-7428-4810-962e-b268bbf66cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import BaseMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7fc61-cca4-4a55-ab4d-836f634444c4",
   "metadata": {},
   "source": [
    "### Create Test Case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4d1ffe-b3f7-4e2e-a63d-3900276a64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_evaluation(queries: list[str],\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      llm: LLM,\n",
    "                      metric: BaseMetric\n",
    "                      ) -> list[dict]:\n",
    "    '''\n",
    "    LLM Evaluation harness that given a list of queries does the following:\n",
    "       1. Retrieves relevant context and reranks\n",
    "       2. Generates evaluated LLM actual output\n",
    "       3. Creates retrieval context for test case\n",
    "       4. Creates a text case on the fly\n",
    "       5. Given a metric execute metric evaluation\n",
    "       6. Returns list of metric evaluations\n",
    "    '''\n",
    "\n",
    "    eval_results = []\n",
    "    for query in tqdm(queries):\n",
    "        try:\n",
    "            result = client.hybrid_search(query, collection_name, limit=200)\n",
    "            reranked = reranker.rerank(result, query, top_k=3)\n",
    "            user_message = generate_prompt_series(query, reranked)\n",
    "            actual_output = llm.chat_completion(huberman_system_message, user_message, temperature=1.0)\n",
    "            retrieval_context = create_context_blocks(reranked)\n",
    "            test_case = LLMTestCase(input=query, actual_output=actual_output, retrieval_context=retrieval_context)\n",
    "            metric.measure(test_case)\n",
    "            # logger.info(test_case.input)\n",
    "            response = EvalResponse(metric=metric,\n",
    "                                    model=azure_eval_model.model,\n",
    "                                    input=test_case.input,\n",
    "                                    actual_output=test_case.actual_output,\n",
    "                                    retrieval_context=test_case.retrieval_context,\n",
    "                                    score=metric.score,\n",
    "                                    reason=metric.reason)\n",
    "            eval_results.append(response)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded51a28-a5d6-42a4-8545-774ecd77211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# eval_results = system_evaluation(random_questions[:5], client, collection_name, llm,faith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02563862-1e64-45f2-a980-663a6b08265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_cases(queries: list[str],\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      llm: LLM,\n",
    "                      ) -> list[LLMTestCase]:\n",
    "    '''\n",
    "    Creates a list of LLM Test Cases based on query retrievals. \n",
    "    '''\n",
    "    results = [client.hybrid_search(query, collection_name, limit=200) for query in tqdm(queries, 'QUERIES')]\n",
    "    reranked = [reranker.rerank(result, queries[i], top_k=3) for i, result in enumerate(tqdm(results, 'RERANKING'))]\n",
    "    user_messages = [generate_prompt_series(queries[i], rerank) for i, rerank in enumerate(reranked)]\n",
    "    actual_outputs = [llm.chat_completion(huberman_system_message, user_message, temperature=1.0) for\n",
    "                      user_message in tqdm(user_messages, 'LLM Calls')]\n",
    "    retrieval_contexts = [create_context_blocks(rerank) for rerank in reranked]\n",
    "    test_cases = [LLMTestCase(input=input, actual_output=output, retrieval_context=context) \\\n",
    "                  for input, output, context in list(zip(queries, actual_outputs, retrieval_contexts))]\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87fe121b-195c-4ff6-83d1-49694baf14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUERIES:   0%|                                                                                                                                                        | 0/15 [00:00<?, ?it/s]/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=142 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/openai/lib/python3.10/site-packages/weaviate/collections/queries/base.py:218: ResourceWarning: unclosed <socket.socket fd=150, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 52898)>\n",
      "  return {\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=150 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/openai/lib/python3.10/site-packages/weaviate/collections/queries/base.py:218: ResourceWarning: unclosed <socket.socket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 52982)>\n",
      "  return {\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=158 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "QUERIES: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.78it/s]\n",
      "RERANKING: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.70it/s]\n",
      "LLM Calls: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:23<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 s, sys: 25.8 ms, total: 6.07 s\n",
      "Wall time: 30.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions = sample(queries, k=25)\n",
    "test_cases = create_test_cases(questions[:15], client, collection_name, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9f953b0-083a-4026-a1b4-75d9ea7de765",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def single_faith_eval(test_case: LLMTestCase,\n",
    "                            model: DeepEvalBaseLLM,\n",
    "                            metric: FaithfulnessMetric | AnswerCorrectnessMetric,\n",
    "                            threshold: float=None\n",
    "                           ) -> EvalResponse:\n",
    "    if metric == FaithfulnessMetric:\n",
    "        threshold = threshold if threshold else 0.5\n",
    "        metric = FaithfulnessMetric(model=model, threshold=threshold)\n",
    "    elif metric == AnswerCorrectnessMetric:\n",
    "        metric = AnswerCorrectnessMetric(model=model).get_metric()\n",
    "    await metric.a_measure(test_case)\n",
    "    response = EvalResponse(metric=metric,\n",
    "                            model=azure_eval_model.model,\n",
    "                            input=test_case.input,\n",
    "                            actual_output=test_case.actual_output,\n",
    "                            retrieval_context=test_case.retrieval_context,\n",
    "                            score=metric.score,\n",
    "                            reason=metric.reason)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00eebff7-0627-4319-bbf2-61384d48ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GEval.a_measure of <deepeval.metrics.g_eval.g_eval.GEval object at 0x7fb6bfc7b4f0>>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnswerCorrectnessMetric(model=azure_eval_model).get_metric().a_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "62d4e4ab-ca04-47af-8a93-8d1a70168791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(deepeval.metrics.faithfulness.faithfulness.FaithfulnessMetric,\n",
       " evaluation.custom_eval_models.AnswerCorrectnessMetric)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FaithfulnessMetric, AnswerCorrectnessMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a8a1275-2c41-405c-bf51-17646fcb68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def asystem_evaluation(\n",
    "                             test_cases: list[LLMTestCase],\n",
    "                             model: DeepEvalBaseLLM,\n",
    "                             metric: FaithfulnessMetric | AnswerCorrectnessMetric,\n",
    "                             threshold: float=None\n",
    "                            ):\n",
    "    tasks = [single_faith_eval(case, model, metric, threshold) for case in test_cases]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55f41085-ed36-4ff1-8caf-c11df3190beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=108 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=108 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=142 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=142 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=105 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=105 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=160 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=160 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=205 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=205 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "&lt;_SelectorSocketTransport fd=206 read=idle write=&lt;idle, bufsize=0&gt;&gt;\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/anaconda/envs/openai/lib/python3.10/asyncio/selector_events.py:710: ResourceWarning: unclosed transport \n",
       "<_SelectorSocketTransport fd=206 read=idle write=<idle, bufsize=0>>\n",
       "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.72 s, sys: 763 ms, total: 9.48 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "responses = asyncio.run(asystem_evaluation(test_cases[:10], azure_eval_model, AnswerCorrectnessMetric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "44f9d041-d51b-40ff-b3c0-4482dbb9b686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "81a717b4-3f19-40c7-b526-bff359e5ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [r.score for r in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc38096f-e520-41ed-9f5a-73f090dd5e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e408cdbf-cad3-464d-ae80-1a4cb4b3dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons = [r.reason for r in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d5ca7c9a-0132-4db0-9378-5be5508c592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of sentence_transformers.util failed: Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: snapshot_download() requires a code object with 0 free vars, not 3\n",
      "]\n",
      "[autoreload of sentence_transformers.SentenceTransformer failed: Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/anaconda/envs/openai/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: save_to_hub() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The actual output correctly identifies adequate sleep and stress management as foundational modulators for engaging tenacity and willpower, as emphasized by Dr. Huberman in the retrieval context. The output reflects the focus on these elements and accurately summarizes the content from the source material, thereby addressing the question effectively and accurately. The detailed reference to Dr. Huberman and his recommendations on sleep reinforce the direct correlation with the highlighted discussion in the podcast. No contradictory information is presented, and the actual output displays a comprehensive understanding of the material. However, a slight mark deduction is applied because the output does not mention the importance of the anterior mid-cingulate cortex and the physiological aspect of glucose availability in the brain, which are also discussed as key elements in the retrieval context.',\n",
       " 'The actual output aligns with the retrieval context by accurately presenting the relationship between quality sleep and the regulation of appetite and metabolism as discussed in the Huberman Lab podcast. It highlights the importance of transitioning through different sleep stages to optimize metabolic processes, which is corroborated by the details found within the retrieval context. Additionally, there is no contradictory information between the provided output and the context from the podcast transcripts.',\n",
       " \"The actual output effectively paraphrases the content from the retrieval context by elucidating how emotional deprivation contrasts with healthy emotional development. It echoes the podcast's emphasis on the developmental processes and neurobiological factors as outlined by Andrew Huberman. The issues of neglect and deprivation covered in the transcripts align with the comparison made in the output, and there is a clear distinction drawn between the two approaches as discussed in the source material. Overall, the response addresses the input query comprehensively without any contradictory or missing information.\",\n",
       " 'The actual output provides a detailed and relevant explanation that is congruent with the information from the retrieval context regarding the use of hypnosis to dissociate somatic reactions from psychological reactions. It accurately reflects the methods described by Dr. David Spiegel, including the visualization technique to manage stress. The response also correctly incorporates facts about the therapeutic benefits of hypnosis and its role in improving mind-body interactions, thus aligning well with the provided context.',\n",
       " \"The actual output aligns with the retrieval context by discussing Dr. Kay Tye's perspective on the impact of social media on the brain, specifically mentioning the amygdala and creative processing. However, while the core idea about the potential negative influence on creativity by social media aligns with the provided context, it lacks the breadth of discussion on additional neural circuits and processes related to creativity, such as the executive network mentioned by Dr. Huberman. The response could be further improved by incorporating more of this broader understanding and insights directly related to neural circuits of creativity beyond the amygdala.\",\n",
       " \"The actual output aligns well with the retrieval context in terms of providing appropriate methods to raise core body temperature before static stretching, as suggested by the expert on the Huberman Lab podcast. It integrates relevant advice from the transcripts such as performing light cardiovascular exercises or calisthenic movements. However, points were deducted because the output's connection to warming up aiding in injury prevention could have been articulated more clearly with direct supporting statements from the transcripts.\",\n",
       " 'The output closely aligns with the retrieval context by accurately listing the peptides and detailing how they mimic melanocyte-stimulating hormone, including their receptor activation and ability to cross the blood-brain barrier. However, the score is not a full 10 due to a minor lack of detail on the effects of mood, libido, and appetite that are mentioned but not fully explored as per the information in the retrieval context.',\n",
       " \"The actual output provides a comprehensive explanation of how caffeine interacts with adenosine receptors in the brain and its consequent effects on wakefulness and sleep. It aligns well with the retrieval context, as the details mentioned by Dr. Huberman about caffeine's role as an antagonist to adenosine, as well as the subsequent increased alertness and eventual rebound sleepiness, are echoed in the actual output. The slight deduction is due to the lack of mention of the dual role of caffeine as both a vasoconstrictor and vasodilator in the context of adenosine, which is a nuanced detail found in the retrieval context that could give a more rounded understanding of caffeine's effects.\",\n",
       " \"The actual output closely aligns with the input requirements, detailing how InsideTracker provides both information and guidance that's distinctive from other DNA and blood tests, with emphasis on actionable advice and personalized wellness plans. It specifically addresses ease of use, personalized directives, and the addition of a health dashboard, which directly responds to the input question on guidance offered by InsideTracker. The retrieval context is relevant and aligns with the output, further confirming details about the comprehensive nature of InsideTracker's services and the unique benefits it offers over other tests. The reason it did not receive a perfect score is due to a lack of information contrasting InsideTracker with 'other DNA tests and blood tests' in a more direct comparative manner which would fully satisfy the input query.\",\n",
       " \"The retrieval context includes information about both anabolic steroids and Palmer cooling and their respective roles in enhancing gym performance. The actual output does not provide a direct comparison but acknowledges the absence of specific comparative information within the provided context, which is truthful given the transcripts. The output is on-point, addressing the non-existence of the comparison in the transcripts while maintaining relevance to the input's query.\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'zT`\\xb2\\x990\\xd2\\xe3\\xaf\\xb8\\t\\x87\\xea\\x9b\\x15Zs% 1\\xe7\\x7f\\x19\\xcd\\xb2t\\xf5\\x10rf\\xfd\\xe0\\x90\\xe9lP\\xa6\\x8fD*\\xc9\\x11\\xb0>\\x0f\\x1a\\xc4\\xa0\\xc2\\xe6\\xd8\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xcex\\xf1\\xd5\\x0f\\x8f\\xad/\\x07\\x13\\x0b\\x19\\x9a\\x0ejk\\x81%\\x90\\xf0\\xa9\\xaa']\n",
      "Bad pipe message: %s [b'U#\\x9b\\xe7<n\\xf570H\\x87\\xe4\\xfd\\x05\\xdf\\xd4\\x18E\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae']\n",
      "Bad pipe message: %s [b\"\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06\"]\n",
      "Bad pipe message: %s [b'3F\\x17\\xf7`\\xce\\xbb\\xfb:\\xf3\\xd5J\\xd2]\\x06!S9\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0']\n",
      "Bad pipe message: %s [b\"W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\"]\n",
      "Bad pipe message: %s [b'\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0']\n",
      "Bad pipe message: %s [b'\\xcc\\x07 \\x83x\\xe4\\x94n\\xce\\x1fE\\xee\\xf58\\x91\\xf7zn\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00', b'\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x15\\xc5\\x96\\xb0\\xfb\\x06\\x03\\xaa\\xfe\\xb8%\\xdf\\x1e\\x8aMc*P\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00']\n",
      "Bad pipe message: %s [b'\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'm\\x90D\\xc6\\rJ\\xae\\xeb\\xb0\\xdc\\xb6]gW\\xbe@R\\xa0\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\xd8Y\\xfdG\\xbd\\x9f\"\\xc6)\\xc0\\xbc\\xf7\\xfb\\x92\\'G\\xab\\x0b\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0', b'\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00']\n",
      "Bad pipe message: %s [b'D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b\"\\xf2?\\\\\\\\\\x15\\x11It\\xda\\xaeh\\xcd\\xfb\\xde7\\xc9Z\\xff\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0\"]\n",
      "Bad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\n",
      "Bad pipe message: %s [b'\\xfaJk\\x95\\xb8\\x0c@\\xcf\\x1bt\\xbd\\xa1\\x15E<Vc\"\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0']\n",
      "Bad pipe message: %s [b'\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0']\n",
      "Bad pipe message: %s [b'#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00']\n",
      "Bad pipe message: %s [b'D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n"
     ]
    }
   ],
   "source": [
    "reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac4ff978-9325-4898-8394-c561ddc3d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorereasons = list(zip(scores, reasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba4c63b7-525a-4cd2-9c3c-789e9fc37c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(data: list[tuple]):\n",
    "    for atuple in data:\n",
    "        print(f'SCORE: {atuple[0]}')\n",
    "        reason = atuple[1][:25]\n",
    "        print(f'REASON: {reason}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733c5c8-a0c2-4a99-a57a-2a1ed5d74839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
