{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c191c675-ff1b-4ff1-b37b-de31cf8ca354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5ea2ba-43d6-4a14-aed3-a4be1c1bf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from src.reranker import ReRanker\n",
    "from src.evaluation.custom_eval_models import AnswerCorrectnessMetric, EvalResponse, CustomAzureOpenAI\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import load_azure_openai\n",
    "from src.llm.prompt_templates import (huberman_system_message, question_answering_prompt_series,\n",
    "                                     create_context_blocks, generate_prompt_series)\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from nest_asyncio import apply\n",
    "from tqdm import tqdm\n",
    "import asyncio \n",
    "apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cec42-7bfe-44eb-bdf9-8ea9df2c5961",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4e5095-e559-4858-93db-c4f1ce22a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/golden_datasets/golden_256.json'\n",
    "data = FileIO().load_json(data_path)\n",
    "queries = list(data['queries'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a507c92-439a-4a0b-bf51-7451be4d46b3",
   "metadata": {},
   "source": [
    "### Set Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17120ee3-7221-43f9-886a-edc729645028",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_weaviate_client()\n",
    "collection_name = 'Huberman_minilm_256'\n",
    "reranker= ReRanker()\n",
    "llm = load_azure_openai()\n",
    "azure_eval_model = CustomAzureOpenAI('graphrag-gpt4-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b162baf-4dbf-42b7-a4b7-b007b0b4757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aget_actual_outputs(user_messages: list[str]):\n",
    "    tasks = [llm.achat_completion(huberman_system_message, user_message, temperature=1.0) for user_message in user_messages]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "219b4163-f476-4e58-81b9-064cce240881",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def acreate_test_cases( queries: list[str],\n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              llm: LLM,\n",
    "                              ) -> list[LLMTestCase]:\n",
    "    '''\n",
    "    Creates a list of LLM Test Cases based on query retrievals. \n",
    "    '''\n",
    "    results = [client.hybrid_search(query, collection_name, limit=200) for query in tqdm(queries, 'QUERIES')]\n",
    "    reranked = [reranker.rerank(result, queries[i], top_k=3) for i, result in enumerate(tqdm(results, 'RERANKING'))]\n",
    "    user_messages = [generate_prompt_series(queries[i], rerank, 1) for i, rerank in enumerate(reranked)]\n",
    "    actual_outputs = await aget_actual_outputs(user_messages)\n",
    "    retrieval_contexts = [create_context_blocks(rerank) for rerank in reranked]\n",
    "    test_cases = [LLMTestCase(input=input, actual_output=output, retrieval_context=context) \\\n",
    "                  for input, output, context in list(zip(queries, actual_outputs, retrieval_contexts))]\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab10428-1ec8-4a4c-bc01-b5ee6eabf5c3",
   "metadata": {},
   "source": [
    "### Get Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d203ec8d-9664-45e7-8888-cb2d6ef79400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUERIES: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.10it/s]\n",
      "RERANKING: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(50)\n",
    "test_cases = await acreate_test_cases(queries, client, collection_name, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f296578-67e3-4a4b-aa5f-55edf53c1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to list of dicts\n",
    "test_case_list = [test_case.__dict__ for test_case in test_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd70c50-36a7-4b0c-9074-25ed64062016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save results to disk\n",
    "# FileIO.save_as_json('../data/test_cases_256.json', test_case_list)\n",
    "test_cases = FileIO.load_json('../data/test_cases_256.json')\n",
    "test_cases = [LLMTestCase(input=tc['input'], actual_output=tc['actual_output'], retrieval_context=tc['retrieval_context'])\n",
    "              for tc in test_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15989cb1-44e4-4b8f-8a9a-56ad3ed78eae",
   "metadata": {},
   "source": [
    "### Launch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc5137c9-7c87-490c-b15f-9a93ed74ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def asingle_eval_call(test_case: LLMTestCase,\n",
    "                            model: str | DeepEvalBaseLLM,\n",
    "                            metric: AnswerCorrectnessMetric,\n",
    "                            threshold: float=None,\n",
    "                            return_context_data: bool=True\n",
    "                            ) -> EvalResponse:\n",
    "    # if metric == FaithfulnessMetric:\n",
    "    #     threshold = threshold if threshold else 0.5\n",
    "    #     metric = FaithfulnessMetric(model=model, threshold=threshold)\n",
    "    if metric == AnswerCorrectnessMetric:\n",
    "        metric = AnswerCorrectnessMetric(model)\n",
    "    await metric.a_measure(test_case)\n",
    "    response = load_eval_response(metric, test_case, return_context_data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e9b2bb8-4094-4833-8196-8ed858ca8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def asystem_evaluation(test_cases: list[LLMTestCase],\n",
    "                             model: DeepEvalBaseLLM,\n",
    "                             metric: AnswerCorrectnessMetric,\n",
    "                             batch_size: int=10,\n",
    "                             threshold: float=None\n",
    "                            ):\n",
    "    from tqdm import tqdm\n",
    "    from math import ceil\n",
    "    completed = []\n",
    "    batches = ceil(len(test_cases)/batch_size)\n",
    "    for i in tqdm(range(batches), 'BATCHES'):\n",
    "        batch = test_cases[i*batch_size:(i+1)*batch_size]\n",
    "        tasks = [asingle_eval_call(case, model, metric, threshold) for case in batch]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        completed.extend(responses)\n",
    "        await asyncio.sleep(30)\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "861ef05a-4853-4440-840c-4c5e780749e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BATCHES:   0%|                                                                                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BATCHES:   0%|                                                                                                                   | 0/5 [00:03<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "responses = await asystem_evaluation(test_cases, azure_eval_model, AnswerCorrectnessMetric, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3ee0a38-907b-4bf1-b629-0e44e23a94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_response(metric, test_case, return_context_data: bool=True):\n",
    "    return EvalResponse(score=metric.score,\n",
    "                        reason=metric.reason,\n",
    "                        metric=metric.__class__.__name__,\n",
    "                        cost=metric.evaluation_cost, \n",
    "                        eval_model=metric.evaluation_model,\n",
    "                        eval_steps=metric.evaluation_steps,\n",
    "                        input=test_case.input if return_context_data else None,\n",
    "                        actual_output=test_case.actual_output if return_context_data else None,\n",
    "                        retrieval_context=None #retrieval_context if return_context_data else None\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a135147d-8d51-414a-9dbe-49925781a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "turbo35_df = pd.DataFrame([r.score for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e20556b5-2c2c-4551-a91c-bfbc755ec014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.269491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean     0.901000\n",
       "std      0.269491\n",
       "min      0.000000\n",
       "25%      1.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbo35_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2477c51a-0312-47ba-9b32-84327d8b347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_df=pd.DataFrame([r.score for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a4ad694-ef9c-4983-8c6f-33545dd96fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.269491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean     0.901000\n",
       "std      0.269491\n",
       "min      0.000000\n",
       "25%      1.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d68c1937-fcb5-49f4-a715-564e87df632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79f69489-afca-4e9a-ac3b-cff7aaec3f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement, comprehensively covers all key aspects mentioned in the input, and accurately explains the practices for deliberately increasing adrenaline while staying calm mentally and their utility in dealing with unwanted events.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How can one deliberately increase adrenaline while staying calm mentally, and what utility does this have in dealing with unwanted events?', actual_output='One can deliberately increase adrenaline while staying calm mentally through practices like breathing techniques and exposure to cold water or psychological stress. By learning to separate the brain-body experience and inducing adrenaline deliberately while maintaining mental calmness, individuals can develop the ability to regulate their stress response. This has great utility in dealing with unwanted events, as it allows one to push back on potential infection from bacteria or viruses and manage the effects of adrenaline in response to stress-inducing situations.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement and is comprehensive in addressing all key aspects mentioned in the input.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='What behavioral approach can be used to deliberately turn off the communication between sensory neurons and motor neurons for pain relief?', actual_output='The behavioral approach that can be used to deliberately turn off the communication between sensory neurons and motor neurons for pain relief is to modulate the activity of modulatory neurons. By adjusting the modulatory neurons, it is possible to cut off the communication between sensory neurons and motor neurons, thereby relaxing the muscles and reducing the perception of pain. Dr. Andrew Huberman discusses how this approach can be used as an alternative to medications or natural supplement-based treatments for pain relief.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The text effectively addresses the specific information requirement stated in the input, has high comprehensiveness and the actual output correlates with the retrieval context.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input=\"How has Tim Ferriss's approach changed since writing The 4-Hour Body, in terms of optimizing and de-optimizing certain areas for well-being?\", actual_output=\"Since writing The 4-Hour Body, Tim Ferriss's approach has shifted from seeking areas to optimize to deliberately de-optimizing certain aspects of his life in order to increase his sense of well-being. He has transitioned from a focus on compulsively productive activities to selectively de-optimizing certain areas, such as stopping measurement, ignoring certain types of information, and deleting distractions like Twitter. This change reflects his emphasis on prioritizing well-being and happiness over constant optimization and productivity.\", retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement, comprehensively covers all key aspects, and aligns with the neuroscience research discussed by Dr. Andrew Huberman.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How can visualizing failure be an effective strategy to motivate oneself when feeling unmotivated towards a goal?', actual_output='Visualizing failure can be an effective strategy to motivate oneself when feeling unmotivated towards a goal because it activates the amygdala, a key component of the neural circuitry involved in goal-oriented behaviors. The amygdala assesses the value of goals and determines the actions necessary to pursue them. By visualizing potential failures and the negative consequences of not achieving the goal, one can tap into the neural circuitry linked to the amygdala, which can provide a motivation boost and drive action toward the goal. This approach aligns with the neuroscience research discussed by Dr. Andrew Huberman and emphasizes the effectiveness of visualizing the potential downsides of not reaching a goal to increase motivation.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output addresses all aspects of the input and provides accurate and comprehensive information about finding a comfortable level when taking betaine HCL pepsin tablets or capsules.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How do people find a comfortable level when taking betaine HCL pepsin tablets or capsules?', actual_output='People find a comfortable level when taking betaine HCL pepsin tablets or capsules by starting slow with one or two tablets or capsules and then experimenting to find a level that does not create an excessive feeling of warmth in the stomach or disrupt digestion. They are advised to assess whether the tablets or capsules improve symptoms of indigestion, mood, wellbeing, and the sensation of their gut viscera. It is important to consult a healthcare provider before exploring this and to monitor how the supplements affect their overall well-being and digestion.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement stated in the input and comprehensively addresses all key aspects mentioned in the input.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How does the level of cortisol released at baseline relate to the effects of alcohol consumption on stress?', actual_output='The level of cortisol released at baseline is influenced by alcohol consumption. Regular alcohol consumption, even at low to moderate levels, can result in increased release of cortisol at baseline when individuals are not drinking. This increased cortisol release leads to heightened levels of stress and anxiety when individuals are not consuming alcohol. Additionally, chronic alcohol intake can lead to changes in neural circuits that may encourage further drinking, thus perpetuating the cycle of stress and alcohol consumption.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement stated in the input. The actual output is comprehensive in addressing all key aspects mentioned in the input.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How can listeners support the podcast in addition to leaving reviews and comments on Spotify and Apple?', actual_output='Listeners can support the podcast by subscribing to it on YouTube, checking out the sponsors mentioned in the episodes, and considering the Patreon page at patreon.com/AndrewHuberman, where they can choose to support the podcast at any level they prefer. Additionally, they can sign up for the Neural Network Newsletter at hubermanlab.com, which provides summaries of podcast episodes and detailed lists of protocols related to them, all at no cost and without sharing any personal information.', retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement stated in the input. The comprehensiveness of the actual output in addressing all key aspects mentioned in the input is high. The information provided is accurate and comprehensive, and the score is not affected by a lack of information in the retrieval context.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='How prevalent is the use of Adderall and Ritalin without diagnosis of ADHD among young people?', actual_output='The use of Adderall and Ritalin without a diagnosis of ADHD among young people is quite prevalent, with up to 25% of college students and perhaps as many as 35% of individuals between the ages of 17 and 30 taking Adderall on a regular or semi-regular basis for studying, working, and daily functioning, despite not having been diagnosed with ADHD. The transcripts from the podcast series indicate that there is a significant black market for these drugs, and many individuals obtain them from people with prescriptions or through other sources. The issue of off-prescription use of these medications is highlighted as a concerning and widespread phenomenon, especially among young adults.', retrieval_context=None),\n",
       " EvalResponse(score=0.9, reason='The actual output effectively addresses the specific information requirement and comprehensively covers all key aspects mentioned in the input. However, the score is not perfect due to minor issues relating to verification of factual accuracy.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='Why is there an increasing consumption of Adderall without prescription compared to cannabis in certain age groups?', actual_output=\"The increasing consumption of Adderall without prescription compared to cannabis in certain age groups is attributed to the high demand for focus and attention in today's society. Andrew Huberman mentions that there are discussions around whether life is becoming more demanding, leading to excessive need for focus. This, in turn, may be contributing to the misuse of stimulants like Adderall for cognitive enhancement. The podcast emphasizes that a high number of young adults and college students are using stimulants to improve their ability to focus, whether it is for academic performance or work-related demands. Furthermore, the off-prescription use of stimulants is fueled by the widespread availability of these drugs through black market sources, contributing to their increased consumption compared to cannabis in certain age groups.\", retrieval_context=None),\n",
       " EvalResponse(score=1.0, reason='The actual output effectively addresses the specific information requirement stated in the input, and is comprehensive in addressing all key aspects mentioned in the input.', metric='AnswerCorrectnessMetric', cost=None, eval_model='Custom Azure OpenAI Model', eval_steps=['Compare the actual output with the retrieval context to verify factual accuracy.', 'Assess if the actual output effectively addresses the specific information requirement stated in the input.', 'Determine the comprehensiveness of the actual output in addressing all key aspects mentioned in the input.', 'Score the actual output between 0 and 1, based on the accuracy and comprehensiveness of the information provided.', 'If there is not enough information in the retrieval context to correctly answer the input, and the actual output indicates that the input cannot be answered with the provided context, then the score should be 1.'], input='What substance and practice combination might enhance serotonergic transmission in the brain in the future?', actual_output='The substance and practice combination that might enhance serotonergic transmission in the brain in the future is the use of a pharmacologic agent that specifically enhances activation of the serotonin 2A receptor in particular brain areas, favoring both divergent and convergent thinking. This pharmacologic agent is referred to as very low dose or microdosing of psilocybin. According to the transcripts, it is suggested that this form of pharmacology can shift brain neurotransmitters and neuromodulators in ways that favor creativity and enhance activation of the serotonergic pathways associated with the 5-HT2A receptor. Additionally, it is mentioned that combining such neurochemical enhancement with practices like gratitude could potentiate the induction of neuroplasticity and create longer-lasting or more robust brain changes.', retrieval_context=None)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0945d-601a-45af-b72d-a3961eaca686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
